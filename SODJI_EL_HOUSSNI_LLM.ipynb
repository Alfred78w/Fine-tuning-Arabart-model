{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alfred78w/Fine-tuning-Arabart-model/blob/main/SODJI_EL_HOUSSNI_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zICNbbfWYTMW"
      },
      "source": [
        "<center><big><big><big><b>Data Challenge</b></big></big></big></center>\n",
        "<center><big><big><big><b>Darija ‚û°Ô∏è MSA Translation</b></big></big></big></center>\n",
        "\n",
        "<center><big><big><i> By:</i></big></big></center>\n",
        "\n",
        "<center><big><big><i>üåü EL HOUSSNI HOUSSAM üåü</i></big></big></center>\n",
        "<center><big><big><i>üåü SODJI SOUROU ALFRED üåü</i></big></big></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnHFhPJgZ2gL"
      },
      "source": [
        "**<big><big>Task B: Data Augmentation**\n",
        "\n",
        "**<big> Step 1: Using online ressources**\n",
        "\n",
        "\n",
        "To enrich our training data for Darija to MSA translation, we will begin by sourcing  vocabulary and texts from online resources and databases.\n",
        "\n",
        "These online resources will provide us with authentic Darija sentences paired with their corresponding MSA translations, enhancing the diversity and quality of our dataset.\n",
        "\n",
        "Sites web of our selected data:\n",
        "\n",
        "https://tajinequiparle.com/en/english-moroccan-arabic-dictionary/a/ (web scraping)\n",
        "\n",
        "https://atlasculturalfoundation.org/wp-content/uploads/2019/02/darija-phrasebook.pdf  (Using Data-->Import from Web, function in Excel)\n",
        "\n",
        "https://www.loecsen.com/en/vocabulary-arabic-moroccan   (Using Data-->Import from Web, function in Excel)\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1JC6LY9MxHwlxDiNM_e9rPAMFJSY0tt6BuRPcNI1Bf-U/edit#gid=91538701\n",
        "\n",
        "\n",
        "After finding the data online, we will compile all the vocabulary encountered from online sources into a clean Excel sheet, ensuring its usability for further data augmentation and model training.\n",
        "\n",
        "**<big> Step 2 : Using LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEvwQpYlUjwr"
      },
      "source": [
        "\n",
        "\n",
        "##Web scraping\n",
        "For database augmentation, one of our key techniques is to take a bilingual dictionary ``[English <---> Darija]`` from online data. On the [tajinequiparle](https://tajinequiparle.com/en/english-moroccan-arabic-dictionary/a/) site there is a bilingual dictionary\n",
        " English <---> Darija]`` which we used to augment our database. So we set up a web scraping algorithm with the library\n",
        " `BeautifulSoup` which allowed us to have a database containing words in `English` and their translation in `Moroccan dialect` of approximately 4K lines. This database named `words.csv` contains `Word` which is the word, `Nature` - the nature of the word, `darija_Translation` - the translation into Moroccan dialect in Latin script, `Arabic_Writing` - the writing of the word in Arabic script, `Meaning`, and `relating to` which are not useful enough in our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v60xyhfdueF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_tajine_qui_parle(lettre):\n",
        "    # Page URL\n",
        "    url = \"https://tajinequiparle.com/en/english-moroccan-arabic-dictionary/\"+ lettre +\"/\"\n",
        "\n",
        "    # Page content retrieval\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Searching for items with the class \"glossary-listing\"\n",
        "    links = soup.find_all(\"a\", class_=\"glossary-listing\")\n",
        "\n",
        "    # Initialisation of the list of URLs\n",
        "    urls = []\n",
        "\n",
        "    # Browse elements to extract links\n",
        "    for link in links:\n",
        "       # Get the URL\n",
        "        href = link.get(\"href\")\n",
        "        # Add the URL to the list\n",
        "        urls.append(href)\n",
        "\n",
        "    # Display the URLs\n",
        "    # print(urls)\n",
        "\n",
        "    # Function to retrieve information about a word page\n",
        "    def get_word_info(url):\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Retrieve the English word\n",
        "        word = soup.find(\"h2\").text.strip() if soup.find(\"h2\") else \"\"\n",
        "\n",
        "        # Retrieve meanings and their translations\n",
        "        natures = []\n",
        "        meanings = []\n",
        "        arabic_translations = []\n",
        "        arabic_writings = []\n",
        "\n",
        "        for h3 in soup.find_all(\"h3\"):\n",
        "            if h3.find(\"em\"):\n",
        "                nature_info = h3.find('em').text.strip()\n",
        "                natures.append(nature_info)\n",
        "\n",
        "        if natures == []: natures.append('')\n",
        "\n",
        "        for h5 in soup.find_all(\"h5\"):\n",
        "            arabic_text = h5.text.strip().split(\"[\", 1)\n",
        "            if len(arabic_text) > 1:\n",
        "                arabic_writing = arabic_text[1].split(\"]\")[0].strip()\n",
        "            else:\n",
        "                arabic_writing = \"\"  # Or any other default value if needed\n",
        "            meaning_text = h5.text.strip().split(\"(\", 1)\n",
        "            if len(meaning_text) > 1:\n",
        "                meaning = meaning_text[1].split(\")\")[0].strip()\n",
        "            else:\n",
        "                meaning = \"\"  # Or any other default value if needed\n",
        "            arabic_translation = \"\"\n",
        "            arabic_translation = h5.find('q').text.strip() if h5.find('q') else(\"\")\n",
        "            meanings.append(meaning)\n",
        "            arabic_translations.append(arabic_translation)\n",
        "            arabic_writings.append(arabic_writing)\n",
        "        return word, natures, meanings, arabic_translations, arabic_writings\n",
        "\n",
        "    nom_fichier_csv = \"words.csv\"\n",
        "\n",
        "    # Open a CSV file to write data in append mode\n",
        "    with open(nom_fichier_csv, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Word', 'Nature', 'darija_Translation', 'Arabic_Writing', 'Meaning', 'relating to']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # Loop through the URLs to retrieve information for each word\n",
        "        for url in urls:\n",
        "            word, natures, meanings, arabic_translations, arabic_writings = get_word_info(\"https://tajinequiparle.com\" + url)\n",
        "            # Write the information into the CSV file\n",
        "            # print(word)\n",
        "            for i in range(len(meanings)):\n",
        "                meaning_split = meanings[i].split(\" \", 1)\n",
        "                if len(meaning_split) > 1:\n",
        "                    meaning = meaning_split[1]\n",
        "                else:\n",
        "                    meaning = meanings[i]  # Use the complete meaning if no split is possible\n",
        "                writer.writerow({'Word': word, 'Nature': natures[i if i < len(natures) else -1],\n",
        "                                'darija_Translation': arabic_translations[i],\n",
        "                                'Arabic_Writing': arabic_writings[i], 'Meaning': meaning,\n",
        "                                'relating to': meaning_split[0]})\n",
        "\n",
        "# scrape_tajine_qui_parle(\"a\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A-0s6bt8mXj"
      },
      "outputs": [],
      "source": [
        "nom_fichier_csv = \"words.csv\"\n",
        "    # Open a CSV file to write data\n",
        "with open(nom_fichier_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['Word', 'Nature', 'darija_Translation', 'Arabic_Writing', 'Meaning', 'relating to']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    # Write the column names in the CSV file\n",
        "    writer.writeheader()\n",
        "\n",
        "letters = [chr(i) for i in range(ord('y'), ord('z')+1)]\n",
        "\n",
        "for letter in letters:\n",
        "    scrape_tajine_qui_parle(letter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVHpqZ9ytvdV"
      },
      "source": [
        "##Data Preprocessing\n",
        "The extracted data undergoes preprocessing to clean and format it according to project requirements. This involve handling missing values and reformatting data into json file.\n",
        "Data cleaning is crucial to eliminate errors, inconsistent formats, and irrelevant information. In our case this includes\n",
        "- Removing duplicates to avoid training bias.\n",
        "- Removing missing values\n",
        "- Correcting typos or grammatical errors in source and target texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RqkrHbmKDl5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUyBwRoebg0n",
        "outputId": "86cc7fb1-f0df-4431-8a36-f5cee5820993"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('test-4000.json', <http.client.HTTPMessage at 0x7f8d4d1b1660>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import urllib\n",
        "\n",
        "urllib.request.urlretrieve(\"https://onedrive.live.com/download?resid=758EF4B4D89BDE45%214789&authkey=!ABCcTMoUFdERYYI\", 'test-4000.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsdqauuZS6JW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = \"/content/our_final_database.xlsx\"\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Remove duplicate rows\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Rename columns to English names\n",
        "df.rename(columns={\"Darija_Writing\": \"Darija\", \"Word\": \"English\"}, inplace=True)\n",
        "\n",
        "# Select only the English, Darija, and MSA columns\n",
        "df = df[[\"English\", \"Darija\",\"MSA\"]]\n",
        "\n",
        "# Convert boolean values to strings\n",
        "df = df.applymap(lambda x: str(x) if isinstance(x, bool) else x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5f-hvsocjZb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "translations = []\n",
        "for idx, row in df.iterrows():\n",
        "    translation_dict = {\n",
        "        \"translation\": {\n",
        "            \"idx\": idx,\n",
        "            \"English\": row[\"English\"],\n",
        "            \"Darija\": row[\"Darija\"],\n",
        "            \"MSA\": row[\"MSA\"]\n",
        "        }\n",
        "    }\n",
        "    translations.append(translation_dict)\n",
        "with open(\"train2.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "    for translation in translations:\n",
        "        json.dump(translation, json_file, ensure_ascii=False)\n",
        "        json_file.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madV9I-ECeKj"
      },
      "source": [
        "# Task C\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "# The model we employ in this assignment is 'Arabart,' an encoder-decoder model. We have opted for Arabart due to the following qualities:\n",
        "\n",
        "1. **Linguistic Specialization**: Arabart is specifically engineered to comprehend and generate Arabic text, which encompasses familiarity with linguistic nuances, syntax, and vocabulary unique to the Arabic language. This renders it particularly suitable for tasks involving Arabic, unlike more generalized models.\n",
        "2. **Dialect Support**: Models like Arabart are trained on a broad spectrum of Arabic texts, including various dialects. This means they are better equipped to handle the specifics of Darija, a Maghrebi Arabic dialect, when translating to or from Modern Standard Arabic (MSA).\n",
        "3. **Translation Quality**: Thanks to its specialized training on Arabic texts, Arabart can provide superior translation quality for language pairs involving Arabic compared to non-specialized models. This includes better management of complex grammatical structures and more precise vocabulary selection.\n",
        "4. **Adaptability**: Arabart, being a member of the BART family of models, is designed for text generation tasks, including translation. This means it can be finely tuned for specific tasks, offering flexibility in its application to particular translation needs.\n",
        "5. **Performance on Specific Tasks**: Models like Arabart have demonstrated excellent performance on Arabic-specific tasks such as automatic translation, summarization, and sentiment detection. This suggests they can offer similar benefits in your translation exercise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-uBPdzZueDN",
        "outputId": "76e38f23-6936-48ae-eace-a82122b24101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 14 00:58:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrXJgQeWtTvg",
        "outputId": "f4897d93-1c87-45a2-8e0d-8ba20e5b807f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 191301, done.\u001b[K\n",
            "remote: Counting objects: 100% (191235/191235), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44857/44857), done.\u001b[K\n",
            "remote: Total 191301 (delta 136280), reused 189480 (delta 135041), pack-reused 66\u001b[K\n",
            "Receiving objects: 100% (191301/191301), 199.29 MiB | 26.19 MiB/s, done.\n",
            "Resolving deltas: 100% (136280/136280), done.\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-n032zz0h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-n032zz0h\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit f738ab3b5d30e30c43a4c3d00ca8939f8a4d4427\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8680419 sha256=dfa88622058f461cbdbe99a7acd57b788f74eda92bf7f2a94d066a0648b4283b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_j31vqh2/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed transformers-4.39.0.dev0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.1-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.8.2 sacrebleu-2.4.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install sentencepiece\n",
        "!pip install tensorboardX\n",
        "!pip install accelerate -U\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIbx8trLtXUM",
        "outputId": "f8d1160d-326c-4238-e8dc-fa1a3e378375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-14 02:10:57.510546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-14 02:10:57.510605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-14 02:10:57.512042: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-14 02:10:58.667394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "03/14/2024 02:11:00 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/14/2024 02:11:00 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=2,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=2,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=200,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./tmp/tst-translation/runs/Mar14_02-11-00_bd28ac963eb0,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=200,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=./tmp/tst-translation,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=32,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./tmp/tst-translation,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=200,\n",
            "save_strategy=steps,\n",
            "save_total_limit=15,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-5183b6e89d608f15\n",
            "03/14/2024 02:11:01 - INFO - datasets.builder - Using custom data configuration default-5183b6e89d608f15\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "03/14/2024 02:11:01 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "03/14/2024 02:11:01 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n",
            "03/14/2024 02:11:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)\n",
            "03/14/2024 02:11:01 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n",
            "03/14/2024 02:11:01 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05\n",
            "[INFO|configuration_utils.py:726] 2024-03-14 02:11:01,548 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--moussaKam--arabart/snapshots/cb67d617b84f52755c0cbf34684a41fe0f01cdb1/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-14 02:11:01,552 >> Model config MBartConfig {\n",
            "  \"_name_or_path\": \"moussaKam/arabart\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"MBartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"transformers_version\": \"4.39.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:626] 2024-03-14 02:11:01,722 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:726] 2024-03-14 02:11:01,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--moussaKam--arabart/snapshots/cb67d617b84f52755c0cbf34684a41fe0f01cdb1/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-14 02:11:01,950 >> Model config MBartConfig {\n",
            "  \"_name_or_path\": \"moussaKam/arabart\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"MBartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"transformers_version\": \"4.39.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2057] 2024-03-14 02:11:02,303 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--moussaKam--arabart/snapshots/cb67d617b84f52755c0cbf34684a41fe0f01cdb1/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2057] 2024-03-14 02:11:02,303 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2057] 2024-03-14 02:11:02,303 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2057] 2024-03-14 02:11:02,303 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2057] 2024-03-14 02:11:02,303 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:726] 2024-03-14 02:11:02,304 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--moussaKam--arabart/snapshots/cb67d617b84f52755c0cbf34684a41fe0f01cdb1/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-14 02:11:02,305 >> Model config MBartConfig {\n",
            "  \"_name_or_path\": \"moussaKam/arabart\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"MBartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"transformers_version\": \"4.39.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50002\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:726] 2024-03-14 02:11:02,468 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--moussaKam--arabart/snapshots/cb67d617b84f52755c0cbf34684a41fe0f01cdb1/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-03-14 02:11:02,469 >> Model config MBartConfig {\n",
            "  \"_name_or_path\": \"moussaKam/arabart\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"MBartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"extra_pos_embeddings\": 2,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"static_position_embeddings\": false,\n",
            "  \"tokenizer_class\": \"BarthezTokenizer\",\n",
            "  \"transformers_version\": \"4.39.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50002\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3279] 2024-03-14 02:11:02,633 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--moussaKam--arabart/snapshots/cb67d617b84f52755c0cbf34684a41fe0f01cdb1/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:927] 2024-03-14 02:11:02,667 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4017] 2024-03-14 02:11:03,170 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:4025] 2024-03-14 02:11:03,171 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at moussaKam/arabart.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:3569] 2024-03-14 02:11:03,339 >> Generation config file not found, using a generation config created from the model config.\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00000_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00001_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00002_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00003_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00004_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00005_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00006_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00007_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_00007_of_00008.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_*_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9389eebfe00b802d_*_of_00008.arrow\n",
            "Concatenating 8 shards\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "Process #0 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00000_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #0 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00000_of_00008.arrow\n",
            "Process #1 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00001_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #1 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00001_of_00008.arrow\n",
            "Process #2 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00002_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #2 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00002_of_00008.arrow\n",
            "Process #3 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00003_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #3 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00003_of_00008.arrow\n",
            "Process #4 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00004_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #4 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00004_of_00008.arrow\n",
            "Process #5 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00005_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #5 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00005_of_00008.arrow\n",
            "Process #6 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00006_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #6 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00006_of_00008.arrow\n",
            "Process #7 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00007_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Process #7 will write at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_00007_of_00008.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_*_of_00008.arrow\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-5183b6e89d608f15/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-533c21694e94567b_*_of_00008.arrow\n",
            "Concatenating 8 shards\n",
            "03/14/2024 02:11:03 - INFO - datasets.arrow_dataset - Concatenating 8 shards\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1826] 2024-03-14 02:11:04,708 >> ***** Running training *****\n",
            "[INFO|trainer.py:1827] 2024-03-14 02:11:04,708 >>   Num examples = 9,771\n",
            "[INFO|trainer.py:1828] 2024-03-14 02:11:04,708 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1829] 2024-03-14 02:11:04,708 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1832] 2024-03-14 02:11:04,708 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1833] 2024-03-14 02:11:04,708 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1834] 2024-03-14 02:11:04,709 >>   Total optimization steps = 1,530\n",
            "[INFO|trainer.py:1835] 2024-03-14 02:11:04,709 >>   Number of trainable parameters = 139,221,504\n",
            "{'loss': 3.4352, 'grad_norm': 4.351605415344238, 'learning_rate': 4.3464052287581704e-05, 'epoch': 0.65}\n",
            " 13% 200/1530 [00:40<04:49,  4.59it/s][INFO|trainer.py:3363] 2024-03-14 02:11:45,119 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:11:45,120 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:11:45,120 >>   Batch size = 32\n",
            "[INFO|configuration_utils.py:927] 2024-03-14 02:11:45,256 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:34,  3.54it/s]\u001b[A\n",
            "  2% 3/125 [00:01<00:55,  2.21it/s]\u001b[A\n",
            "  3% 4/125 [00:01<01:06,  1.81it/s]\u001b[A\n",
            "  4% 5/125 [00:02<01:13,  1.63it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:16,  1.56it/s]\u001b[A\n",
            "  6% 7/125 [00:03<01:13,  1.60it/s]\u001b[A\n",
            "  6% 8/125 [00:04<01:17,  1.52it/s]\u001b[A\n",
            "  7% 9/125 [00:05<01:13,  1.57it/s]\u001b[A\n",
            "  8% 10/125 [00:05<01:11,  1.60it/s]\u001b[A\n",
            "  9% 11/125 [00:06<01:14,  1.54it/s]\u001b[A\n",
            " 10% 12/125 [00:07<01:11,  1.58it/s]\u001b[A\n",
            " 10% 13/125 [00:07<01:14,  1.51it/s]\u001b[A\n",
            " 11% 14/125 [00:08<01:21,  1.36it/s]\u001b[A\n",
            " 12% 15/125 [00:09<01:28,  1.24it/s]\u001b[A\n",
            " 13% 16/125 [00:10<01:37,  1.11it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:41,  1.06it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:37,  1.09it/s]\u001b[A\n",
            " 15% 19/125 [00:13<01:29,  1.18it/s]\u001b[A\n",
            " 16% 20/125 [00:14<01:24,  1.24it/s]\u001b[A\n",
            " 17% 21/125 [00:14<01:22,  1.27it/s]\u001b[A\n",
            " 18% 22/125 [00:15<01:19,  1.29it/s]\u001b[A\n",
            " 18% 23/125 [00:16<01:15,  1.36it/s]\u001b[A\n",
            " 19% 24/125 [00:17<01:14,  1.36it/s]\u001b[A\n",
            " 20% 25/125 [00:17<01:10,  1.41it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:09,  1.43it/s]\u001b[A\n",
            " 22% 27/125 [00:19<01:10,  1.40it/s]\u001b[A\n",
            " 22% 28/125 [00:19<01:07,  1.44it/s]\u001b[A\n",
            " 23% 29/125 [00:20<00:55,  1.73it/s]\u001b[A\n",
            " 24% 30/125 [00:20<00:59,  1.61it/s]\u001b[A\n",
            " 25% 31/125 [00:21<01:02,  1.50it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:03,  1.47it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:09,  1.33it/s]\u001b[A\n",
            " 27% 34/125 [00:24<01:09,  1.30it/s]\u001b[A\n",
            " 28% 35/125 [00:24<01:08,  1.31it/s]\u001b[A\n",
            " 29% 36/125 [00:25<00:58,  1.53it/s]\u001b[A\n",
            " 30% 37/125 [00:25<00:52,  1.69it/s]\u001b[A\n",
            " 30% 38/125 [00:25<00:44,  1.94it/s]\u001b[A\n",
            " 31% 39/125 [00:26<00:43,  1.96it/s]\u001b[A\n",
            " 32% 40/125 [00:26<00:41,  2.05it/s]\u001b[A\n",
            " 33% 41/125 [00:27<00:38,  2.16it/s]\u001b[A\n",
            " 34% 42/125 [00:27<00:36,  2.28it/s]\u001b[A\n",
            " 34% 43/125 [00:28<00:42,  1.95it/s]\u001b[A\n",
            " 35% 44/125 [00:29<00:47,  1.71it/s]\u001b[A\n",
            " 36% 45/125 [00:29<00:49,  1.62it/s]\u001b[A\n",
            " 37% 46/125 [00:30<00:50,  1.55it/s]\u001b[A\n",
            " 38% 47/125 [00:31<00:53,  1.47it/s]\u001b[A\n",
            " 38% 48/125 [00:31<00:52,  1.47it/s]\u001b[A\n",
            " 39% 49/125 [00:32<00:53,  1.43it/s]\u001b[A\n",
            " 40% 50/125 [00:33<00:54,  1.39it/s]\u001b[A\n",
            " 41% 51/125 [00:34<00:53,  1.38it/s]\u001b[A\n",
            " 42% 52/125 [00:35<00:56,  1.29it/s]\u001b[A\n",
            " 42% 53/125 [00:36<01:01,  1.18it/s]\u001b[A\n",
            " 43% 54/125 [00:37<01:02,  1.13it/s]\u001b[A\n",
            " 44% 55/125 [00:37<00:58,  1.20it/s]\u001b[A\n",
            " 45% 56/125 [00:38<00:56,  1.23it/s]\u001b[A\n",
            " 46% 57/125 [00:39<00:53,  1.28it/s]\u001b[A\n",
            " 46% 58/125 [00:40<00:50,  1.32it/s]\u001b[A\n",
            " 47% 59/125 [00:40<00:48,  1.36it/s]\u001b[A\n",
            " 48% 60/125 [00:41<00:57,  1.13it/s]\u001b[A\n",
            " 49% 61/125 [00:43<01:03,  1.00it/s]\u001b[A\n",
            " 50% 62/125 [00:43<00:57,  1.10it/s]\u001b[A\n",
            " 50% 63/125 [00:44<00:54,  1.15it/s]\u001b[A\n",
            " 51% 64/125 [00:45<00:50,  1.20it/s]\u001b[A\n",
            " 52% 65/125 [00:46<00:47,  1.25it/s]\u001b[A\n",
            " 53% 66/125 [00:46<00:47,  1.25it/s]\u001b[A\n",
            " 54% 67/125 [00:47<00:42,  1.36it/s]\u001b[A\n",
            " 54% 68/125 [00:48<00:41,  1.39it/s]\u001b[A\n",
            " 55% 69/125 [00:48<00:38,  1.47it/s]\u001b[A\n",
            " 56% 70/125 [00:49<00:33,  1.64it/s]\u001b[A\n",
            " 57% 71/125 [00:49<00:29,  1.83it/s]\u001b[A\n",
            " 58% 72/125 [00:50<00:27,  1.95it/s]\u001b[A\n",
            " 58% 73/125 [00:50<00:24,  2.09it/s]\u001b[A\n",
            " 59% 74/125 [00:50<00:24,  2.10it/s]\u001b[A\n",
            " 60% 75/125 [00:51<00:22,  2.20it/s]\u001b[A\n",
            " 61% 76/125 [00:51<00:22,  2.14it/s]\u001b[A\n",
            " 62% 77/125 [00:52<00:22,  2.17it/s]\u001b[A\n",
            " 62% 78/125 [00:52<00:20,  2.28it/s]\u001b[A\n",
            " 63% 79/125 [00:53<00:22,  2.08it/s]\u001b[A\n",
            " 64% 80/125 [00:53<00:20,  2.16it/s]\u001b[A\n",
            " 65% 81/125 [00:54<00:24,  1.80it/s]\u001b[A\n",
            " 66% 82/125 [00:55<00:26,  1.64it/s]\u001b[A\n",
            " 66% 83/125 [00:56<00:32,  1.29it/s]\u001b[A\n",
            " 67% 84/125 [00:57<00:30,  1.33it/s]\u001b[A\n",
            " 68% 85/125 [00:57<00:29,  1.36it/s]\u001b[A\n",
            " 69% 86/125 [00:58<00:28,  1.37it/s]\u001b[A\n",
            " 70% 87/125 [00:59<00:29,  1.30it/s]\u001b[A\n",
            " 70% 88/125 [01:00<00:29,  1.27it/s]\u001b[A\n",
            " 71% 89/125 [01:01<00:29,  1.20it/s]\u001b[A\n",
            " 72% 90/125 [01:01<00:28,  1.24it/s]\u001b[A\n",
            " 73% 91/125 [01:02<00:26,  1.29it/s]\u001b[A\n",
            " 74% 92/125 [01:03<00:22,  1.44it/s]\u001b[A\n",
            " 74% 93/125 [01:03<00:22,  1.45it/s]\u001b[A\n",
            " 75% 94/125 [01:04<00:21,  1.45it/s]\u001b[A\n",
            " 76% 95/125 [01:05<00:21,  1.43it/s]\u001b[A\n",
            " 77% 96/125 [01:05<00:20,  1.40it/s]\u001b[A\n",
            " 78% 97/125 [01:06<00:19,  1.40it/s]\u001b[A\n",
            " 78% 98/125 [01:07<00:19,  1.41it/s]\u001b[A\n",
            " 79% 99/125 [01:08<00:18,  1.40it/s]\u001b[A\n",
            " 80% 100/125 [01:08<00:18,  1.36it/s]\u001b[A\n",
            " 81% 101/125 [01:09<00:17,  1.37it/s]\u001b[A\n",
            " 82% 102/125 [01:10<00:16,  1.40it/s]\u001b[A\n",
            " 82% 103/125 [01:10<00:15,  1.40it/s]\u001b[A\n",
            " 83% 104/125 [01:11<00:16,  1.28it/s]\u001b[A\n",
            " 84% 105/125 [01:12<00:15,  1.30it/s]\u001b[A\n",
            " 85% 106/125 [01:13<00:15,  1.25it/s]\u001b[A\n",
            " 86% 107/125 [01:14<00:13,  1.29it/s]\u001b[A\n",
            " 86% 108/125 [01:14<00:12,  1.31it/s]\u001b[A\n",
            " 87% 109/125 [01:15<00:11,  1.36it/s]\u001b[A\n",
            " 88% 110/125 [01:16<00:10,  1.38it/s]\u001b[A\n",
            " 89% 111/125 [01:17<00:10,  1.35it/s]\u001b[A\n",
            " 90% 112/125 [01:17<00:09,  1.36it/s]\u001b[A\n",
            " 90% 113/125 [01:18<00:08,  1.39it/s]\u001b[A\n",
            " 91% 114/125 [01:19<00:08,  1.36it/s]\u001b[A\n",
            " 92% 115/125 [01:19<00:07,  1.38it/s]\u001b[A\n",
            " 93% 116/125 [01:20<00:06,  1.38it/s]\u001b[A\n",
            " 94% 117/125 [01:21<00:05,  1.38it/s]\u001b[A\n",
            " 94% 118/125 [01:22<00:05,  1.39it/s]\u001b[A\n",
            " 95% 119/125 [01:22<00:04,  1.40it/s]\u001b[A\n",
            " 96% 120/125 [01:23<00:03,  1.46it/s]\u001b[A\n",
            " 97% 121/125 [01:24<00:03,  1.32it/s]\u001b[A\n",
            " 98% 122/125 [01:25<00:02,  1.16it/s]\u001b[A\n",
            " 98% 123/125 [01:26<00:01,  1.23it/s]\u001b[A\n",
            " 99% 124/125 [01:26<00:00,  1.32it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.842028856277466, 'eval_bleu': 3.7253, 'eval_gen_len': 10.1275, 'eval_runtime': 89.214, 'eval_samples_per_second': 44.836, 'eval_steps_per_second': 1.401, 'epoch': 0.65}\n",
            " 13% 200/1530 [02:09<04:49,  4.59it/s]\n",
            "100% 125/125 [01:27<00:00,  1.32it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:13:14,335 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-200\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:13:14,335 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:13:14,337 >> Configuration saved in ./tmp/tst-translation/checkpoint-200/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:13:14,338 >> Configuration saved in ./tmp/tst-translation/checkpoint-200/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:13:19,332 >> Model weights saved in ./tmp/tst-translation/checkpoint-200/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:13:19,333 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:13:19,334 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-200/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:13:23,961 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-1800] due to args.save_total_limit\n",
            "{'loss': 2.7188, 'grad_norm': 3.8803343772888184, 'learning_rate': 3.6928104575163405e-05, 'epoch': 1.31}\n",
            " 26% 400/1530 [03:01<03:52,  4.86it/s][INFO|trainer.py:3363] 2024-03-14 02:14:06,252 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:14:06,252 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:14:06,252 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:30,  4.01it/s]\u001b[A\n",
            "  2% 3/125 [00:01<00:53,  2.27it/s]\u001b[A\n",
            "  3% 4/125 [00:01<01:05,  1.84it/s]\u001b[A\n",
            "  4% 5/125 [00:02<01:12,  1.66it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:12,  1.64it/s]\u001b[A\n",
            "  6% 7/125 [00:04<01:22,  1.42it/s]\u001b[A\n",
            "  6% 8/125 [00:05<01:33,  1.25it/s]\u001b[A\n",
            "  7% 9/125 [00:05<01:30,  1.29it/s]\u001b[A\n",
            "  8% 10/125 [00:06<01:40,  1.15it/s]\u001b[A\n",
            "  9% 11/125 [00:07<01:30,  1.25it/s]\u001b[A\n",
            " 10% 12/125 [00:08<01:27,  1.29it/s]\u001b[A\n",
            " 10% 13/125 [00:09<01:24,  1.32it/s]\u001b[A\n",
            " 11% 14/125 [00:09<01:22,  1.35it/s]\u001b[A\n",
            " 12% 15/125 [00:10<01:19,  1.38it/s]\u001b[A\n",
            " 13% 16/125 [00:11<01:18,  1.40it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:18,  1.38it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:19,  1.35it/s]\u001b[A\n",
            " 15% 19/125 [00:13<01:16,  1.38it/s]\u001b[A\n",
            " 16% 20/125 [00:14<01:15,  1.39it/s]\u001b[A\n",
            " 17% 21/125 [00:14<01:16,  1.37it/s]\u001b[A\n",
            " 18% 22/125 [00:15<01:15,  1.36it/s]\u001b[A\n",
            " 18% 23/125 [00:16<01:19,  1.29it/s]\u001b[A\n",
            " 19% 24/125 [00:17<01:25,  1.19it/s]\u001b[A\n",
            " 20% 25/125 [00:18<01:20,  1.24it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:16,  1.29it/s]\u001b[A\n",
            " 22% 27/125 [00:19<01:15,  1.29it/s]\u001b[A\n",
            " 22% 28/125 [00:20<01:11,  1.35it/s]\u001b[A\n",
            " 23% 29/125 [00:20<00:58,  1.64it/s]\u001b[A\n",
            " 24% 30/125 [00:21<01:01,  1.55it/s]\u001b[A\n",
            " 25% 31/125 [00:22<01:03,  1.48it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:03,  1.45it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:03,  1.46it/s]\u001b[A\n",
            " 27% 34/125 [00:24<00:59,  1.52it/s]\u001b[A\n",
            " 28% 35/125 [00:24<00:57,  1.56it/s]\u001b[A\n",
            " 29% 36/125 [00:25<00:51,  1.72it/s]\u001b[A\n",
            " 30% 37/125 [00:25<00:47,  1.87it/s]\u001b[A\n",
            " 30% 38/125 [00:25<00:40,  2.14it/s]\u001b[A\n",
            " 31% 39/125 [00:26<00:40,  2.14it/s]\u001b[A\n",
            " 32% 40/125 [00:26<00:40,  2.08it/s]\u001b[A\n",
            " 33% 41/125 [00:27<00:39,  2.12it/s]\u001b[A\n",
            " 34% 42/125 [00:27<00:36,  2.26it/s]\u001b[A\n",
            " 34% 43/125 [00:28<00:44,  1.84it/s]\u001b[A\n",
            " 35% 44/125 [00:29<00:55,  1.45it/s]\u001b[A\n",
            " 36% 45/125 [00:30<00:58,  1.36it/s]\u001b[A\n",
            " 37% 46/125 [00:30<00:56,  1.40it/s]\u001b[A\n",
            " 38% 47/125 [00:31<00:57,  1.36it/s]\u001b[A\n",
            " 38% 48/125 [00:32<00:53,  1.43it/s]\u001b[A\n",
            " 39% 49/125 [00:33<00:54,  1.39it/s]\u001b[A\n",
            " 40% 50/125 [00:33<00:55,  1.35it/s]\u001b[A\n",
            " 41% 51/125 [00:34<00:55,  1.34it/s]\u001b[A\n",
            " 42% 52/125 [00:35<00:54,  1.33it/s]\u001b[A\n",
            " 42% 53/125 [00:36<00:55,  1.30it/s]\u001b[A\n",
            " 43% 54/125 [00:37<00:55,  1.27it/s]\u001b[A\n",
            " 44% 55/125 [00:37<00:52,  1.32it/s]\u001b[A\n",
            " 45% 56/125 [00:38<00:52,  1.30it/s]\u001b[A\n",
            " 46% 57/125 [00:39<00:52,  1.31it/s]\u001b[A\n",
            " 46% 58/125 [00:40<00:55,  1.22it/s]\u001b[A\n",
            " 47% 59/125 [00:41<00:59,  1.11it/s]\u001b[A\n",
            " 48% 60/125 [00:42<01:03,  1.02it/s]\u001b[A\n",
            " 49% 61/125 [00:43<01:01,  1.04it/s]\u001b[A\n",
            " 50% 62/125 [00:44<00:56,  1.12it/s]\u001b[A\n",
            " 50% 63/125 [00:44<00:53,  1.16it/s]\u001b[A\n",
            " 51% 64/125 [00:45<00:50,  1.21it/s]\u001b[A\n",
            " 52% 65/125 [00:46<00:48,  1.25it/s]\u001b[A\n",
            " 53% 66/125 [00:47<00:45,  1.29it/s]\u001b[A\n",
            " 54% 67/125 [00:47<00:39,  1.45it/s]\u001b[A\n",
            " 54% 68/125 [00:48<00:34,  1.63it/s]\u001b[A\n",
            " 55% 69/125 [00:48<00:30,  1.85it/s]\u001b[A\n",
            " 56% 70/125 [00:48<00:27,  2.00it/s]\u001b[A\n",
            " 57% 71/125 [00:49<00:26,  2.05it/s]\u001b[A\n",
            " 58% 72/125 [00:49<00:25,  2.10it/s]\u001b[A\n",
            " 58% 73/125 [00:50<00:23,  2.21it/s]\u001b[A\n",
            " 59% 74/125 [00:50<00:23,  2.14it/s]\u001b[A\n",
            " 60% 75/125 [00:51<00:22,  2.27it/s]\u001b[A\n",
            " 61% 76/125 [00:51<00:22,  2.13it/s]\u001b[A\n",
            " 62% 77/125 [00:51<00:21,  2.22it/s]\u001b[A\n",
            " 62% 78/125 [00:52<00:19,  2.44it/s]\u001b[A\n",
            " 63% 79/125 [00:52<00:19,  2.37it/s]\u001b[A\n",
            " 64% 80/125 [00:53<00:19,  2.30it/s]\u001b[A\n",
            " 65% 81/125 [00:54<00:25,  1.71it/s]\u001b[A\n",
            " 66% 82/125 [00:55<00:31,  1.37it/s]\u001b[A\n",
            " 66% 83/125 [00:55<00:31,  1.35it/s]\u001b[A\n",
            " 67% 84/125 [00:56<00:30,  1.36it/s]\u001b[A\n",
            " 68% 85/125 [00:57<00:29,  1.36it/s]\u001b[A\n",
            " 69% 86/125 [00:58<00:28,  1.36it/s]\u001b[A\n",
            " 70% 87/125 [00:58<00:27,  1.38it/s]\u001b[A\n",
            " 70% 88/125 [00:59<00:25,  1.45it/s]\u001b[A\n",
            " 71% 89/125 [01:00<00:24,  1.44it/s]\u001b[A\n",
            " 72% 90/125 [01:00<00:24,  1.42it/s]\u001b[A\n",
            " 73% 91/125 [01:01<00:23,  1.47it/s]\u001b[A\n",
            " 74% 92/125 [01:02<00:20,  1.58it/s]\u001b[A\n",
            " 74% 93/125 [01:02<00:20,  1.59it/s]\u001b[A\n",
            " 75% 94/125 [01:03<00:20,  1.53it/s]\u001b[A\n",
            " 76% 95/125 [01:04<00:20,  1.46it/s]\u001b[A\n",
            " 77% 96/125 [01:04<00:20,  1.43it/s]\u001b[A\n",
            " 78% 97/125 [01:05<00:21,  1.33it/s]\u001b[A\n",
            " 78% 98/125 [01:06<00:21,  1.24it/s]\u001b[A\n",
            " 79% 99/125 [01:07<00:21,  1.19it/s]\u001b[A\n",
            " 80% 100/125 [01:08<00:20,  1.22it/s]\u001b[A\n",
            " 81% 101/125 [01:09<00:18,  1.27it/s]\u001b[A\n",
            " 82% 102/125 [01:09<00:16,  1.37it/s]\u001b[A\n",
            " 82% 103/125 [01:10<00:15,  1.38it/s]\u001b[A\n",
            " 83% 104/125 [01:11<00:16,  1.24it/s]\u001b[A\n",
            " 84% 105/125 [01:11<00:14,  1.38it/s]\u001b[A\n",
            " 85% 106/125 [01:12<00:13,  1.39it/s]\u001b[A\n",
            " 86% 107/125 [01:13<00:12,  1.39it/s]\u001b[A\n",
            " 86% 108/125 [01:14<00:12,  1.39it/s]\u001b[A\n",
            " 87% 109/125 [01:14<00:11,  1.45it/s]\u001b[A\n",
            " 88% 110/125 [01:15<00:10,  1.42it/s]\u001b[A\n",
            " 89% 111/125 [01:16<00:10,  1.38it/s]\u001b[A\n",
            " 90% 112/125 [01:16<00:09,  1.39it/s]\u001b[A\n",
            " 90% 113/125 [01:17<00:08,  1.40it/s]\u001b[A\n",
            " 91% 114/125 [01:18<00:08,  1.28it/s]\u001b[A\n",
            " 92% 115/125 [01:19<00:08,  1.19it/s]\u001b[A\n",
            " 93% 116/125 [01:20<00:07,  1.24it/s]\u001b[A\n",
            " 94% 117/125 [01:20<00:06,  1.28it/s]\u001b[A\n",
            " 94% 118/125 [01:21<00:05,  1.32it/s]\u001b[A\n",
            " 95% 119/125 [01:22<00:04,  1.34it/s]\u001b[A\n",
            " 96% 120/125 [01:23<00:03,  1.42it/s]\u001b[A\n",
            " 97% 121/125 [01:23<00:02,  1.38it/s]\u001b[A\n",
            " 98% 122/125 [01:24<00:02,  1.36it/s]\u001b[A\n",
            " 98% 123/125 [01:25<00:01,  1.37it/s]\u001b[A\n",
            " 99% 124/125 [01:25<00:00,  1.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.644761085510254, 'eval_bleu': 5.3324, 'eval_gen_len': 9.762, 'eval_runtime': 87.9989, 'eval_samples_per_second': 45.455, 'eval_steps_per_second': 1.42, 'epoch': 1.31}\n",
            " 26% 400/1530 [04:29<03:52,  4.86it/s]\n",
            "100% 125/125 [01:27<00:00,  1.36it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:15:34,253 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-400\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:15:34,254 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:15:34,255 >> Configuration saved in ./tmp/tst-translation/checkpoint-400/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:15:34,255 >> Configuration saved in ./tmp/tst-translation/checkpoint-400/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:15:39,435 >> Model weights saved in ./tmp/tst-translation/checkpoint-400/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:15:39,436 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:15:39,436 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-400/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:15:44,057 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-200] due to args.save_total_limit\n",
            "{'loss': 2.376, 'grad_norm': 4.101885795593262, 'learning_rate': 3.0392156862745097e-05, 'epoch': 1.96}\n",
            " 39% 600/1530 [05:22<03:24,  4.55it/s][INFO|trainer.py:3363] 2024-03-14 02:16:27,296 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:16:27,296 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:16:27,296 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:32,  3.74it/s]\u001b[A\n",
            "  2% 3/125 [00:01<00:54,  2.23it/s]\u001b[A\n",
            "  3% 4/125 [00:01<01:06,  1.83it/s]\u001b[A\n",
            "  4% 5/125 [00:02<01:13,  1.64it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:16,  1.56it/s]\u001b[A\n",
            "  6% 7/125 [00:04<01:18,  1.50it/s]\u001b[A\n",
            "  6% 8/125 [00:04<01:19,  1.47it/s]\u001b[A\n",
            "  7% 9/125 [00:05<01:16,  1.51it/s]\u001b[A\n",
            "  8% 10/125 [00:06<01:17,  1.48it/s]\u001b[A\n",
            "  9% 11/125 [00:06<01:16,  1.50it/s]\u001b[A\n",
            " 10% 12/125 [00:07<01:16,  1.47it/s]\u001b[A\n",
            " 10% 13/125 [00:08<01:17,  1.44it/s]\u001b[A\n",
            " 11% 14/125 [00:09<01:22,  1.34it/s]\u001b[A\n",
            " 12% 15/125 [00:09<01:24,  1.29it/s]\u001b[A\n",
            " 13% 16/125 [00:10<01:29,  1.22it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:26,  1.25it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:24,  1.27it/s]\u001b[A\n",
            " 15% 19/125 [00:12<01:18,  1.35it/s]\u001b[A\n",
            " 16% 20/125 [00:13<01:17,  1.36it/s]\u001b[A\n",
            " 17% 21/125 [00:14<01:17,  1.35it/s]\u001b[A\n",
            " 18% 22/125 [00:15<01:17,  1.33it/s]\u001b[A\n",
            " 18% 23/125 [00:15<01:15,  1.36it/s]\u001b[A\n",
            " 19% 24/125 [00:16<01:14,  1.36it/s]\u001b[A\n",
            " 20% 25/125 [00:17<01:11,  1.40it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:09,  1.42it/s]\u001b[A\n",
            " 22% 27/125 [00:18<01:10,  1.38it/s]\u001b[A\n",
            " 22% 28/125 [00:19<01:07,  1.44it/s]\u001b[A\n",
            " 23% 29/125 [00:19<00:55,  1.73it/s]\u001b[A\n",
            " 24% 30/125 [00:20<00:59,  1.60it/s]\u001b[A\n",
            " 25% 31/125 [00:21<01:07,  1.39it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:14,  1.25it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:13,  1.26it/s]\u001b[A\n",
            " 27% 34/125 [00:23<01:06,  1.37it/s]\u001b[A\n",
            " 28% 35/125 [00:24<01:03,  1.41it/s]\u001b[A\n",
            " 29% 36/125 [00:24<00:56,  1.58it/s]\u001b[A\n",
            " 30% 37/125 [00:25<01:02,  1.41it/s]\u001b[A\n",
            " 30% 38/125 [00:26<00:51,  1.70it/s]\u001b[A\n",
            " 31% 39/125 [00:26<00:47,  1.82it/s]\u001b[A\n",
            " 32% 40/125 [00:26<00:43,  1.95it/s]\u001b[A\n",
            " 33% 41/125 [00:27<00:41,  2.02it/s]\u001b[A\n",
            " 34% 42/125 [00:27<00:39,  2.12it/s]\u001b[A\n",
            " 34% 43/125 [00:28<00:42,  1.93it/s]\u001b[A\n",
            " 35% 44/125 [00:29<00:48,  1.65it/s]\u001b[A\n",
            " 36% 45/125 [00:29<00:50,  1.59it/s]\u001b[A\n",
            " 37% 46/125 [00:30<00:51,  1.54it/s]\u001b[A\n",
            " 38% 47/125 [00:31<00:53,  1.45it/s]\u001b[A\n",
            " 38% 48/125 [00:32<00:51,  1.49it/s]\u001b[A\n",
            " 39% 49/125 [00:32<00:52,  1.45it/s]\u001b[A\n",
            " 40% 50/125 [00:33<00:57,  1.29it/s]\u001b[A\n",
            " 41% 51/125 [00:34<01:03,  1.17it/s]\u001b[A\n",
            " 42% 52/125 [00:35<01:00,  1.21it/s]\u001b[A\n",
            " 42% 53/125 [00:36<00:59,  1.22it/s]\u001b[A\n",
            " 43% 54/125 [00:37<00:58,  1.22it/s]\u001b[A\n",
            " 44% 55/125 [00:37<00:54,  1.28it/s]\u001b[A\n",
            " 45% 56/125 [00:38<00:54,  1.28it/s]\u001b[A\n",
            " 46% 57/125 [00:39<00:51,  1.31it/s]\u001b[A\n",
            " 46% 58/125 [00:40<00:49,  1.35it/s]\u001b[A\n",
            " 47% 59/125 [00:40<00:47,  1.38it/s]\u001b[A\n",
            " 48% 60/125 [00:41<00:47,  1.38it/s]\u001b[A\n",
            " 49% 61/125 [00:42<00:46,  1.37it/s]\u001b[A\n",
            " 50% 62/125 [00:42<00:45,  1.38it/s]\u001b[A\n",
            " 50% 63/125 [00:43<00:46,  1.33it/s]\u001b[A\n",
            " 51% 64/125 [00:44<00:44,  1.36it/s]\u001b[A\n",
            " 52% 65/125 [00:45<00:46,  1.30it/s]\u001b[A\n",
            " 53% 66/125 [00:46<00:47,  1.24it/s]\u001b[A\n",
            " 54% 67/125 [00:46<00:44,  1.30it/s]\u001b[A\n",
            " 54% 68/125 [00:47<00:39,  1.46it/s]\u001b[A\n",
            " 55% 69/125 [00:47<00:33,  1.67it/s]\u001b[A\n",
            " 56% 70/125 [00:48<00:30,  1.82it/s]\u001b[A\n",
            " 57% 71/125 [00:48<00:28,  1.92it/s]\u001b[A\n",
            " 58% 72/125 [00:49<00:26,  1.97it/s]\u001b[A\n",
            " 58% 73/125 [00:49<00:24,  2.12it/s]\u001b[A\n",
            " 59% 74/125 [00:50<00:24,  2.06it/s]\u001b[A\n",
            " 60% 75/125 [00:50<00:22,  2.26it/s]\u001b[A\n",
            " 61% 76/125 [00:50<00:23,  2.12it/s]\u001b[A\n",
            " 62% 77/125 [00:51<00:22,  2.12it/s]\u001b[A\n",
            " 62% 78/125 [00:51<00:20,  2.31it/s]\u001b[A\n",
            " 63% 79/125 [00:52<00:19,  2.33it/s]\u001b[A\n",
            " 64% 80/125 [00:52<00:19,  2.32it/s]\u001b[A\n",
            " 65% 81/125 [00:53<00:22,  1.92it/s]\u001b[A\n",
            " 66% 82/125 [00:54<00:25,  1.71it/s]\u001b[A\n",
            " 66% 83/125 [00:54<00:26,  1.57it/s]\u001b[A\n",
            " 67% 84/125 [00:55<00:27,  1.51it/s]\u001b[A\n",
            " 68% 85/125 [00:56<00:27,  1.48it/s]\u001b[A\n",
            " 69% 86/125 [00:56<00:26,  1.46it/s]\u001b[A\n",
            " 70% 87/125 [00:57<00:29,  1.30it/s]\u001b[A\n",
            " 70% 88/125 [00:58<00:29,  1.24it/s]\u001b[A\n",
            " 71% 89/125 [00:59<00:29,  1.24it/s]\u001b[A\n",
            " 72% 90/125 [01:00<00:27,  1.28it/s]\u001b[A\n",
            " 73% 91/125 [01:00<00:25,  1.35it/s]\u001b[A\n",
            " 74% 92/125 [01:01<00:22,  1.47it/s]\u001b[A\n",
            " 74% 93/125 [01:02<00:20,  1.54it/s]\u001b[A\n",
            " 75% 94/125 [01:02<00:20,  1.52it/s]\u001b[A\n",
            " 76% 95/125 [01:03<00:20,  1.46it/s]\u001b[A\n",
            " 77% 96/125 [01:04<00:20,  1.43it/s]\u001b[A\n",
            " 78% 97/125 [01:04<00:18,  1.49it/s]\u001b[A\n",
            " 78% 98/125 [01:05<00:18,  1.47it/s]\u001b[A\n",
            " 79% 99/125 [01:06<00:18,  1.43it/s]\u001b[A\n",
            " 80% 100/125 [01:07<00:17,  1.40it/s]\u001b[A\n",
            " 81% 101/125 [01:07<00:17,  1.41it/s]\u001b[A\n",
            " 82% 102/125 [01:08<00:16,  1.40it/s]\u001b[A\n",
            " 82% 103/125 [01:09<00:17,  1.28it/s]\u001b[A\n",
            " 83% 104/125 [01:10<00:18,  1.16it/s]\u001b[A\n",
            " 84% 105/125 [01:11<00:16,  1.19it/s]\u001b[A\n",
            " 85% 106/125 [01:12<00:17,  1.08it/s]\u001b[A\n",
            " 86% 107/125 [01:13<00:15,  1.15it/s]\u001b[A\n",
            " 86% 108/125 [01:13<00:13,  1.21it/s]\u001b[A\n",
            " 87% 109/125 [01:14<00:12,  1.30it/s]\u001b[A\n",
            " 88% 110/125 [01:15<00:11,  1.32it/s]\u001b[A\n",
            " 89% 111/125 [01:15<00:10,  1.32it/s]\u001b[A\n",
            " 90% 112/125 [01:16<00:09,  1.34it/s]\u001b[A\n",
            " 90% 113/125 [01:17<00:08,  1.42it/s]\u001b[A\n",
            " 91% 114/125 [01:18<00:07,  1.40it/s]\u001b[A\n",
            " 92% 115/125 [01:18<00:07,  1.39it/s]\u001b[A\n",
            " 93% 116/125 [01:19<00:06,  1.38it/s]\u001b[A\n",
            " 94% 117/125 [01:20<00:05,  1.39it/s]\u001b[A\n",
            " 94% 118/125 [01:20<00:05,  1.40it/s]\u001b[A\n",
            " 95% 119/125 [01:21<00:04,  1.39it/s]\u001b[A\n",
            " 96% 120/125 [01:22<00:03,  1.45it/s]\u001b[A\n",
            " 97% 121/125 [01:23<00:03,  1.28it/s]\u001b[A\n",
            " 98% 122/125 [01:24<00:02,  1.17it/s]\u001b[A\n",
            " 98% 123/125 [01:25<00:01,  1.18it/s]\u001b[A\n",
            " 99% 124/125 [01:25<00:00,  1.26it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.5498814582824707, 'eval_bleu': 6.7249, 'eval_gen_len': 9.8395, 'eval_runtime': 88.0972, 'eval_samples_per_second': 45.404, 'eval_steps_per_second': 1.419, 'epoch': 1.96}\n",
            " 39% 600/1530 [06:50<03:24,  4.55it/s]\n",
            "100% 125/125 [01:27<00:00,  1.27it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:17:55,395 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-600\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:17:55,395 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:17:55,396 >> Configuration saved in ./tmp/tst-translation/checkpoint-600/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:17:55,396 >> Configuration saved in ./tmp/tst-translation/checkpoint-600/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:18:00,336 >> Model weights saved in ./tmp/tst-translation/checkpoint-600/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:18:00,337 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:18:00,337 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-600/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:18:04,831 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-400] due to args.save_total_limit\n",
            "{'loss': 2.0882, 'grad_norm': 3.9462966918945312, 'learning_rate': 2.38562091503268e-05, 'epoch': 2.61}\n",
            " 52% 800/1530 [07:43<02:22,  5.12it/s][INFO|trainer.py:3363] 2024-03-14 02:18:47,757 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:18:47,757 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:18:47,757 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:34,  3.58it/s]\u001b[A\n",
            "  2% 3/125 [00:01<00:56,  2.17it/s]\u001b[A\n",
            "  3% 4/125 [00:01<01:07,  1.79it/s]\u001b[A\n",
            "  4% 5/125 [00:02<01:14,  1.62it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:20,  1.48it/s]\u001b[A\n",
            "  6% 7/125 [00:04<01:30,  1.30it/s]\u001b[A\n",
            "  6% 8/125 [00:05<01:35,  1.22it/s]\u001b[A\n",
            "  7% 9/125 [00:06<01:27,  1.32it/s]\u001b[A\n",
            "  8% 10/125 [00:06<01:24,  1.35it/s]\u001b[A\n",
            "  9% 11/125 [00:07<01:19,  1.43it/s]\u001b[A\n",
            " 10% 12/125 [00:08<01:19,  1.43it/s]\u001b[A\n",
            " 10% 13/125 [00:08<01:18,  1.42it/s]\u001b[A\n",
            " 11% 14/125 [00:09<01:18,  1.42it/s]\u001b[A\n",
            " 12% 15/125 [00:10<01:15,  1.45it/s]\u001b[A\n",
            " 13% 16/125 [00:10<01:15,  1.44it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:17,  1.39it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:18,  1.36it/s]\u001b[A\n",
            " 15% 19/125 [00:13<01:16,  1.38it/s]\u001b[A\n",
            " 16% 20/125 [00:13<01:15,  1.38it/s]\u001b[A\n",
            " 17% 21/125 [00:14<01:16,  1.36it/s]\u001b[A\n",
            " 18% 22/125 [00:15<01:18,  1.31it/s]\u001b[A\n",
            " 18% 23/125 [00:16<01:20,  1.27it/s]\u001b[A\n",
            " 19% 24/125 [00:17<01:27,  1.15it/s]\u001b[A\n",
            " 20% 25/125 [00:17<01:20,  1.24it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:17,  1.27it/s]\u001b[A\n",
            " 22% 27/125 [00:19<01:16,  1.28it/s]\u001b[A\n",
            " 22% 28/125 [00:20<01:11,  1.35it/s]\u001b[A\n",
            " 23% 29/125 [00:20<00:59,  1.62it/s]\u001b[A\n",
            " 24% 30/125 [00:21<01:02,  1.53it/s]\u001b[A\n",
            " 25% 31/125 [00:21<01:04,  1.45it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:06,  1.40it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:04,  1.44it/s]\u001b[A\n",
            " 27% 34/125 [00:23<01:00,  1.50it/s]\u001b[A\n",
            " 28% 35/125 [00:24<00:59,  1.51it/s]\u001b[A\n",
            " 29% 36/125 [00:25<00:52,  1.71it/s]\u001b[A\n",
            " 30% 37/125 [00:25<00:48,  1.82it/s]\u001b[A\n",
            " 30% 38/125 [00:25<00:41,  2.10it/s]\u001b[A\n",
            " 31% 39/125 [00:26<00:40,  2.13it/s]\u001b[A\n",
            " 32% 40/125 [00:26<00:39,  2.15it/s]\u001b[A\n",
            " 33% 41/125 [00:27<00:38,  2.21it/s]\u001b[A\n",
            " 34% 42/125 [00:27<00:40,  2.07it/s]\u001b[A\n",
            " 34% 43/125 [00:28<00:48,  1.69it/s]\u001b[A\n",
            " 35% 44/125 [00:29<00:59,  1.37it/s]\u001b[A\n",
            " 36% 45/125 [00:30<00:58,  1.38it/s]\u001b[A\n",
            " 37% 46/125 [00:30<00:56,  1.41it/s]\u001b[A\n",
            " 38% 47/125 [00:31<00:57,  1.36it/s]\u001b[A\n",
            " 38% 48/125 [00:32<00:54,  1.42it/s]\u001b[A\n",
            " 39% 49/125 [00:33<00:54,  1.40it/s]\u001b[A\n",
            " 40% 50/125 [00:33<00:55,  1.36it/s]\u001b[A\n",
            " 41% 51/125 [00:34<00:54,  1.35it/s]\u001b[A\n",
            " 42% 52/125 [00:35<00:54,  1.35it/s]\u001b[A\n",
            " 42% 53/125 [00:36<00:54,  1.31it/s]\u001b[A\n",
            " 43% 54/125 [00:37<00:56,  1.26it/s]\u001b[A\n",
            " 44% 55/125 [00:37<00:54,  1.29it/s]\u001b[A\n",
            " 45% 56/125 [00:38<00:53,  1.29it/s]\u001b[A\n",
            " 46% 57/125 [00:39<00:51,  1.32it/s]\u001b[A\n",
            " 46% 58/125 [00:40<00:54,  1.24it/s]\u001b[A\n",
            " 47% 59/125 [00:41<00:56,  1.17it/s]\u001b[A\n",
            " 48% 60/125 [00:42<00:55,  1.18it/s]\u001b[A\n",
            " 49% 61/125 [00:42<00:52,  1.23it/s]\u001b[A\n",
            " 50% 62/125 [00:43<00:49,  1.27it/s]\u001b[A\n",
            " 50% 63/125 [00:44<00:48,  1.27it/s]\u001b[A\n",
            " 51% 64/125 [00:45<00:47,  1.29it/s]\u001b[A\n",
            " 52% 65/125 [00:45<00:45,  1.31it/s]\u001b[A\n",
            " 53% 66/125 [00:46<00:44,  1.33it/s]\u001b[A\n",
            " 54% 67/125 [00:46<00:38,  1.50it/s]\u001b[A\n",
            " 54% 68/125 [00:47<00:34,  1.65it/s]\u001b[A\n",
            " 55% 69/125 [00:47<00:29,  1.87it/s]\u001b[A\n",
            " 56% 70/125 [00:48<00:27,  1.97it/s]\u001b[A\n",
            " 57% 71/125 [00:48<00:26,  2.06it/s]\u001b[A\n",
            " 58% 72/125 [00:49<00:32,  1.65it/s]\u001b[A\n",
            " 58% 73/125 [00:49<00:27,  1.89it/s]\u001b[A\n",
            " 59% 74/125 [00:50<00:26,  1.92it/s]\u001b[A\n",
            " 60% 75/125 [00:50<00:23,  2.12it/s]\u001b[A\n",
            " 61% 76/125 [00:51<00:24,  2.03it/s]\u001b[A\n",
            " 62% 77/125 [00:51<00:24,  1.93it/s]\u001b[A\n",
            " 62% 78/125 [00:52<00:23,  2.02it/s]\u001b[A\n",
            " 63% 79/125 [00:52<00:24,  1.87it/s]\u001b[A\n",
            " 64% 80/125 [00:53<00:24,  1.87it/s]\u001b[A\n",
            " 65% 81/125 [00:54<00:26,  1.64it/s]\u001b[A\n",
            " 66% 82/125 [00:54<00:27,  1.55it/s]\u001b[A\n",
            " 66% 83/125 [00:55<00:28,  1.46it/s]\u001b[A\n",
            " 67% 84/125 [00:56<00:28,  1.43it/s]\u001b[A\n",
            " 68% 85/125 [00:57<00:28,  1.42it/s]\u001b[A\n",
            " 69% 86/125 [00:57<00:27,  1.41it/s]\u001b[A\n",
            " 70% 87/125 [00:58<00:27,  1.38it/s]\u001b[A\n",
            " 70% 88/125 [00:59<00:25,  1.43it/s]\u001b[A\n",
            " 71% 89/125 [01:00<00:25,  1.42it/s]\u001b[A\n",
            " 72% 90/125 [01:00<00:25,  1.40it/s]\u001b[A\n",
            " 73% 91/125 [01:01<00:23,  1.44it/s]\u001b[A\n",
            " 74% 92/125 [01:01<00:21,  1.54it/s]\u001b[A\n",
            " 74% 93/125 [01:02<00:20,  1.56it/s]\u001b[A\n",
            " 75% 94/125 [01:03<00:19,  1.61it/s]\u001b[A\n",
            " 76% 95/125 [01:04<00:20,  1.44it/s]\u001b[A\n",
            " 77% 96/125 [01:04<00:22,  1.30it/s]\u001b[A\n",
            " 78% 97/125 [01:05<00:23,  1.21it/s]\u001b[A\n",
            " 78% 98/125 [01:06<00:21,  1.26it/s]\u001b[A\n",
            " 79% 99/125 [01:07<00:20,  1.30it/s]\u001b[A\n",
            " 80% 100/125 [01:08<00:19,  1.30it/s]\u001b[A\n",
            " 81% 101/125 [01:08<00:17,  1.35it/s]\u001b[A\n",
            " 82% 102/125 [01:09<00:16,  1.37it/s]\u001b[A\n",
            " 82% 103/125 [01:10<00:15,  1.38it/s]\u001b[A\n",
            " 83% 104/125 [01:10<00:15,  1.37it/s]\u001b[A\n",
            " 84% 105/125 [01:11<00:13,  1.49it/s]\u001b[A\n",
            " 85% 106/125 [01:12<00:12,  1.47it/s]\u001b[A\n",
            " 86% 107/125 [01:12<00:12,  1.44it/s]\u001b[A\n",
            " 86% 108/125 [01:13<00:11,  1.43it/s]\u001b[A\n",
            " 87% 109/125 [01:14<00:10,  1.47it/s]\u001b[A\n",
            " 88% 110/125 [01:15<00:10,  1.44it/s]\u001b[A\n",
            " 89% 111/125 [01:15<00:10,  1.39it/s]\u001b[A\n",
            " 90% 112/125 [01:16<00:10,  1.28it/s]\u001b[A\n",
            " 90% 113/125 [01:17<00:09,  1.21it/s]\u001b[A\n",
            " 91% 114/125 [01:18<00:09,  1.20it/s]\u001b[A\n",
            " 92% 115/125 [01:19<00:07,  1.26it/s]\u001b[A\n",
            " 93% 116/125 [01:19<00:06,  1.31it/s]\u001b[A\n",
            " 94% 117/125 [01:20<00:06,  1.33it/s]\u001b[A\n",
            " 94% 118/125 [01:21<00:05,  1.35it/s]\u001b[A\n",
            " 95% 119/125 [01:22<00:04,  1.36it/s]\u001b[A\n",
            " 96% 120/125 [01:22<00:03,  1.43it/s]\u001b[A\n",
            " 97% 121/125 [01:23<00:02,  1.37it/s]\u001b[A\n",
            " 98% 122/125 [01:24<00:02,  1.34it/s]\u001b[A\n",
            " 98% 123/125 [01:24<00:01,  1.37it/s]\u001b[A\n",
            " 99% 124/125 [01:25<00:00,  1.38it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.567732810974121, 'eval_bleu': 7.4872, 'eval_gen_len': 10.0852, 'eval_runtime': 87.7094, 'eval_samples_per_second': 45.605, 'eval_steps_per_second': 1.425, 'epoch': 2.61}\n",
            " 52% 800/1530 [09:10<02:22,  5.12it/s]\n",
            "100% 125/125 [01:26<00:00,  1.35it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:20:15,468 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-800\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:20:15,468 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:20:15,469 >> Configuration saved in ./tmp/tst-translation/checkpoint-800/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:20:15,470 >> Configuration saved in ./tmp/tst-translation/checkpoint-800/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:20:20,624 >> Model weights saved in ./tmp/tst-translation/checkpoint-800/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:20:20,625 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-800/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:20:20,625 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-800/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:20:25,233 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-800] due to args.save_total_limit\n",
            "{'loss': 1.9747, 'grad_norm': 3.6595919132232666, 'learning_rate': 1.7320261437908496e-05, 'epoch': 3.27}\n",
            " 65% 1000/1530 [10:03<01:59,  4.43it/s][INFO|trainer.py:3363] 2024-03-14 02:21:07,983 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:21:07,983 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:21:07,983 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:31,  3.85it/s]\u001b[A\n",
            "  2% 3/125 [00:01<00:53,  2.30it/s]\u001b[A\n",
            "  3% 4/125 [00:01<01:05,  1.84it/s]\u001b[A\n",
            "  4% 5/125 [00:02<01:10,  1.70it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:13,  1.62it/s]\u001b[A\n",
            "  6% 7/125 [00:03<01:16,  1.54it/s]\u001b[A\n",
            "  6% 8/125 [00:04<01:17,  1.51it/s]\u001b[A\n",
            "  7% 9/125 [00:05<01:13,  1.58it/s]\u001b[A\n",
            "  8% 10/125 [00:05<01:11,  1.60it/s]\u001b[A\n",
            "  9% 11/125 [00:06<01:10,  1.62it/s]\u001b[A\n",
            " 10% 12/125 [00:07<01:12,  1.56it/s]\u001b[A\n",
            " 10% 13/125 [00:07<01:15,  1.49it/s]\u001b[A\n",
            " 11% 14/125 [00:08<01:14,  1.48it/s]\u001b[A\n",
            " 12% 15/125 [00:09<01:17,  1.42it/s]\u001b[A\n",
            " 13% 16/125 [00:10<01:43,  1.05it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:36,  1.11it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:32,  1.16it/s]\u001b[A\n",
            " 15% 19/125 [00:13<01:26,  1.23it/s]\u001b[A\n",
            " 16% 20/125 [00:13<01:22,  1.27it/s]\u001b[A\n",
            " 17% 21/125 [00:14<01:20,  1.29it/s]\u001b[A\n",
            " 18% 22/125 [00:15<01:19,  1.29it/s]\u001b[A\n",
            " 18% 23/125 [00:16<01:16,  1.34it/s]\u001b[A\n",
            " 19% 24/125 [00:16<01:14,  1.35it/s]\u001b[A\n",
            " 20% 25/125 [00:17<01:10,  1.43it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:09,  1.43it/s]\u001b[A\n",
            " 22% 27/125 [00:18<01:10,  1.39it/s]\u001b[A\n",
            " 22% 28/125 [00:19<01:08,  1.42it/s]\u001b[A\n",
            " 23% 29/125 [00:19<00:55,  1.72it/s]\u001b[A\n",
            " 24% 30/125 [00:20<01:00,  1.58it/s]\u001b[A\n",
            " 25% 31/125 [00:21<01:05,  1.43it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:11,  1.30it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:13,  1.26it/s]\u001b[A\n",
            " 27% 34/125 [00:23<01:05,  1.39it/s]\u001b[A\n",
            " 28% 35/125 [00:24<00:59,  1.52it/s]\u001b[A\n",
            " 29% 36/125 [00:24<00:52,  1.68it/s]\u001b[A\n",
            " 30% 37/125 [00:25<00:48,  1.81it/s]\u001b[A\n",
            " 30% 38/125 [00:25<00:41,  2.11it/s]\u001b[A\n",
            " 31% 39/125 [00:25<00:39,  2.15it/s]\u001b[A\n",
            " 32% 40/125 [00:26<00:38,  2.20it/s]\u001b[A\n",
            " 33% 41/125 [00:26<00:37,  2.25it/s]\u001b[A\n",
            " 34% 42/125 [00:27<00:36,  2.28it/s]\u001b[A\n",
            " 34% 43/125 [00:27<00:40,  2.00it/s]\u001b[A\n",
            " 35% 44/125 [00:28<00:47,  1.71it/s]\u001b[A\n",
            " 36% 45/125 [00:29<00:49,  1.63it/s]\u001b[A\n",
            " 37% 46/125 [00:29<00:50,  1.57it/s]\u001b[A\n",
            " 38% 47/125 [00:30<00:53,  1.47it/s]\u001b[A\n",
            " 38% 48/125 [00:31<00:50,  1.53it/s]\u001b[A\n",
            " 39% 49/125 [00:32<00:51,  1.47it/s]\u001b[A\n",
            " 40% 50/125 [00:32<00:53,  1.40it/s]\u001b[A\n",
            " 41% 51/125 [00:33<00:57,  1.28it/s]\u001b[A\n",
            " 42% 52/125 [00:34<01:01,  1.18it/s]\u001b[A\n",
            " 42% 53/125 [00:35<01:03,  1.14it/s]\u001b[A\n",
            " 43% 54/125 [00:36<01:01,  1.16it/s]\u001b[A\n",
            " 44% 55/125 [00:37<00:56,  1.23it/s]\u001b[A\n",
            " 45% 56/125 [00:38<00:55,  1.23it/s]\u001b[A\n",
            " 46% 57/125 [00:38<00:53,  1.27it/s]\u001b[A\n",
            " 46% 58/125 [00:39<00:51,  1.30it/s]\u001b[A\n",
            " 47% 59/125 [00:40<00:49,  1.33it/s]\u001b[A\n",
            " 48% 60/125 [00:41<00:48,  1.33it/s]\u001b[A\n",
            " 49% 61/125 [00:41<00:47,  1.33it/s]\u001b[A\n",
            " 50% 62/125 [00:42<00:46,  1.35it/s]\u001b[A\n",
            " 50% 63/125 [00:43<00:47,  1.31it/s]\u001b[A\n",
            " 51% 64/125 [00:44<00:46,  1.31it/s]\u001b[A\n",
            " 52% 65/125 [00:44<00:45,  1.33it/s]\u001b[A\n",
            " 53% 66/125 [00:45<00:46,  1.28it/s]\u001b[A\n",
            " 54% 67/125 [00:46<00:43,  1.35it/s]\u001b[A\n",
            " 54% 68/125 [00:46<00:39,  1.44it/s]\u001b[A\n",
            " 55% 69/125 [00:47<00:36,  1.55it/s]\u001b[A\n",
            " 56% 70/125 [00:47<00:31,  1.74it/s]\u001b[A\n",
            " 57% 71/125 [00:48<00:29,  1.85it/s]\u001b[A\n",
            " 58% 72/125 [00:48<00:27,  1.90it/s]\u001b[A\n",
            " 58% 73/125 [00:49<00:24,  2.10it/s]\u001b[A\n",
            " 59% 74/125 [00:49<00:23,  2.14it/s]\u001b[A\n",
            " 60% 75/125 [00:49<00:21,  2.31it/s]\u001b[A\n",
            " 61% 76/125 [00:50<00:22,  2.16it/s]\u001b[A\n",
            " 62% 77/125 [00:50<00:21,  2.19it/s]\u001b[A\n",
            " 62% 78/125 [00:51<00:20,  2.31it/s]\u001b[A\n",
            " 63% 79/125 [00:51<00:20,  2.25it/s]\u001b[A\n",
            " 64% 80/125 [00:52<00:19,  2.33it/s]\u001b[A\n",
            " 65% 81/125 [00:52<00:23,  1.89it/s]\u001b[A\n",
            " 66% 82/125 [00:53<00:25,  1.70it/s]\u001b[A\n",
            " 66% 83/125 [00:54<00:27,  1.55it/s]\u001b[A\n",
            " 67% 84/125 [00:55<00:27,  1.50it/s]\u001b[A\n",
            " 68% 85/125 [00:55<00:27,  1.47it/s]\u001b[A\n",
            " 69% 86/125 [00:56<00:26,  1.44it/s]\u001b[A\n",
            " 70% 87/125 [00:57<00:26,  1.42it/s]\u001b[A\n",
            " 70% 88/125 [00:58<00:26,  1.39it/s]\u001b[A\n",
            " 71% 89/125 [00:58<00:28,  1.28it/s]\u001b[A\n",
            " 72% 90/125 [00:59<00:28,  1.22it/s]\u001b[A\n",
            " 73% 91/125 [01:00<00:26,  1.27it/s]\u001b[A\n",
            " 74% 92/125 [01:01<00:23,  1.40it/s]\u001b[A\n",
            " 74% 93/125 [01:01<00:21,  1.47it/s]\u001b[A\n",
            " 75% 94/125 [01:02<00:20,  1.51it/s]\u001b[A\n",
            " 76% 95/125 [01:03<00:20,  1.45it/s]\u001b[A\n",
            " 77% 96/125 [01:03<00:20,  1.41it/s]\u001b[A\n",
            " 78% 97/125 [01:04<00:19,  1.41it/s]\u001b[A\n",
            " 78% 98/125 [01:05<00:18,  1.42it/s]\u001b[A\n",
            " 79% 99/125 [01:05<00:18,  1.43it/s]\u001b[A\n",
            " 80% 100/125 [01:06<00:17,  1.40it/s]\u001b[A\n",
            " 81% 101/125 [01:07<00:16,  1.41it/s]\u001b[A\n",
            " 82% 102/125 [01:08<00:16,  1.41it/s]\u001b[A\n",
            " 82% 103/125 [01:08<00:15,  1.41it/s]\u001b[A\n",
            " 83% 104/125 [01:09<00:15,  1.40it/s]\u001b[A\n",
            " 84% 105/125 [01:10<00:14,  1.40it/s]\u001b[A\n",
            " 85% 106/125 [01:11<00:14,  1.27it/s]\u001b[A\n",
            " 86% 107/125 [01:12<00:14,  1.25it/s]\u001b[A\n",
            " 86% 108/125 [01:12<00:12,  1.31it/s]\u001b[A\n",
            " 87% 109/125 [01:13<00:11,  1.39it/s]\u001b[A\n",
            " 88% 110/125 [01:14<00:10,  1.39it/s]\u001b[A\n",
            " 89% 111/125 [01:14<00:10,  1.37it/s]\u001b[A\n",
            " 90% 112/125 [01:15<00:09,  1.37it/s]\u001b[A\n",
            " 90% 113/125 [01:16<00:08,  1.45it/s]\u001b[A\n",
            " 91% 114/125 [01:16<00:07,  1.41it/s]\u001b[A\n",
            " 92% 115/125 [01:17<00:07,  1.41it/s]\u001b[A\n",
            " 93% 116/125 [01:18<00:06,  1.42it/s]\u001b[A\n",
            " 94% 117/125 [01:18<00:05,  1.43it/s]\u001b[A\n",
            " 94% 118/125 [01:19<00:04,  1.42it/s]\u001b[A\n",
            " 95% 119/125 [01:20<00:04,  1.41it/s]\u001b[A\n",
            " 96% 120/125 [01:21<00:03,  1.48it/s]\u001b[A\n",
            " 97% 121/125 [01:22<00:03,  1.20it/s]\u001b[A\n",
            " 98% 122/125 [01:23<00:02,  1.15it/s]\u001b[A\n",
            " 98% 123/125 [01:24<00:01,  1.13it/s]\u001b[A\n",
            " 99% 124/125 [01:24<00:00,  1.22it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.5422751903533936, 'eval_bleu': 7.5541, 'eval_gen_len': 9.6933, 'eval_runtime': 86.8171, 'eval_samples_per_second': 46.074, 'eval_steps_per_second': 1.44, 'epoch': 3.27}\n",
            " 65% 1000/1530 [11:30<01:59,  4.43it/s]\n",
            "100% 125/125 [01:26<00:00,  1.25it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:22:34,801 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-1000\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:22:34,802 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:22:34,803 >> Configuration saved in ./tmp/tst-translation/checkpoint-1000/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:22:34,803 >> Configuration saved in ./tmp/tst-translation/checkpoint-1000/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:22:38,710 >> Model weights saved in ./tmp/tst-translation/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:22:38,711 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:22:38,712 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:22:43,151 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-600] due to args.save_total_limit\n",
            "{'loss': 1.8837, 'grad_norm': 3.880742311477661, 'learning_rate': 1.0784313725490197e-05, 'epoch': 3.92}\n",
            " 78% 1200/1530 [12:21<01:07,  4.87it/s][INFO|trainer.py:3363] 2024-03-14 02:23:26,433 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:23:26,434 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:23:26,434 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:33,  3.68it/s]\u001b[A\n",
            "  2% 3/125 [00:01<00:54,  2.24it/s]\u001b[A\n",
            "  3% 4/125 [00:01<01:06,  1.83it/s]\u001b[A\n",
            "  4% 5/125 [00:02<01:12,  1.66it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:12,  1.64it/s]\u001b[A\n",
            "  6% 7/125 [00:04<01:16,  1.55it/s]\u001b[A\n",
            "  6% 8/125 [00:04<01:18,  1.48it/s]\u001b[A\n",
            "  7% 9/125 [00:05<01:18,  1.47it/s]\u001b[A\n",
            "  8% 10/125 [00:06<01:22,  1.40it/s]\u001b[A\n",
            "  9% 11/125 [00:07<01:27,  1.31it/s]\u001b[A\n",
            " 10% 12/125 [00:07<01:22,  1.37it/s]\u001b[A\n",
            " 10% 13/125 [00:08<01:21,  1.38it/s]\u001b[A\n",
            " 11% 14/125 [00:09<01:20,  1.38it/s]\u001b[A\n",
            " 12% 15/125 [00:09<01:18,  1.40it/s]\u001b[A\n",
            " 13% 16/125 [00:10<01:17,  1.41it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:18,  1.38it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:19,  1.35it/s]\u001b[A\n",
            " 15% 19/125 [00:12<01:17,  1.37it/s]\u001b[A\n",
            " 16% 20/125 [00:13<01:17,  1.36it/s]\u001b[A\n",
            " 17% 21/125 [00:14<01:17,  1.35it/s]\u001b[A\n",
            " 18% 22/125 [00:15<01:17,  1.33it/s]\u001b[A\n",
            " 18% 23/125 [00:15<01:14,  1.36it/s]\u001b[A\n",
            " 19% 24/125 [00:16<01:14,  1.36it/s]\u001b[A\n",
            " 20% 25/125 [00:17<01:10,  1.41it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:16,  1.30it/s]\u001b[A\n",
            " 22% 27/125 [00:19<01:22,  1.19it/s]\u001b[A\n",
            " 22% 28/125 [00:19<01:18,  1.23it/s]\u001b[A\n",
            " 23% 29/125 [00:20<01:03,  1.51it/s]\u001b[A\n",
            " 24% 30/125 [00:20<01:05,  1.46it/s]\u001b[A\n",
            " 25% 31/125 [00:21<01:06,  1.42it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:06,  1.40it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:03,  1.45it/s]\u001b[A\n",
            " 27% 34/125 [00:23<01:00,  1.51it/s]\u001b[A\n",
            " 28% 35/125 [00:24<00:56,  1.59it/s]\u001b[A\n",
            " 29% 36/125 [00:24<00:51,  1.74it/s]\u001b[A\n",
            " 30% 37/125 [00:25<00:47,  1.85it/s]\u001b[A\n",
            " 30% 38/125 [00:25<00:41,  2.10it/s]\u001b[A\n",
            " 31% 39/125 [00:25<00:40,  2.12it/s]\u001b[A\n",
            " 32% 40/125 [00:26<00:39,  2.14it/s]\u001b[A\n",
            " 33% 41/125 [00:26<00:37,  2.24it/s]\u001b[A\n",
            " 34% 42/125 [00:27<00:36,  2.27it/s]\u001b[A\n",
            " 34% 43/125 [00:27<00:40,  2.01it/s]\u001b[A\n",
            " 35% 44/125 [00:28<00:47,  1.71it/s]\u001b[A\n",
            " 36% 45/125 [00:29<00:48,  1.65it/s]\u001b[A\n",
            " 37% 46/125 [00:30<00:55,  1.43it/s]\u001b[A\n",
            " 38% 47/125 [00:31<01:03,  1.23it/s]\u001b[A\n",
            " 38% 48/125 [00:31<00:58,  1.31it/s]\u001b[A\n",
            " 39% 49/125 [00:32<00:57,  1.31it/s]\u001b[A\n",
            " 40% 50/125 [00:33<00:57,  1.30it/s]\u001b[A\n",
            " 41% 51/125 [00:34<00:56,  1.31it/s]\u001b[A\n",
            " 42% 52/125 [00:34<00:55,  1.32it/s]\u001b[A\n",
            " 42% 53/125 [00:35<00:55,  1.29it/s]\u001b[A\n",
            " 43% 54/125 [00:36<00:55,  1.27it/s]\u001b[A\n",
            " 44% 55/125 [00:37<00:53,  1.31it/s]\u001b[A\n",
            " 45% 56/125 [00:38<00:53,  1.30it/s]\u001b[A\n",
            " 46% 57/125 [00:38<00:51,  1.32it/s]\u001b[A\n",
            " 46% 58/125 [00:39<00:49,  1.35it/s]\u001b[A\n",
            " 47% 59/125 [00:40<00:48,  1.35it/s]\u001b[A\n",
            " 48% 60/125 [00:40<00:48,  1.35it/s]\u001b[A\n",
            " 49% 61/125 [00:41<00:49,  1.30it/s]\u001b[A\n",
            " 50% 62/125 [00:42<00:50,  1.24it/s]\u001b[A\n",
            " 50% 63/125 [00:44<01:02,  1.00s/it]\u001b[A\n",
            " 51% 64/125 [00:44<00:56,  1.08it/s]\u001b[A\n",
            " 52% 65/125 [00:45<00:52,  1.14it/s]\u001b[A\n",
            " 53% 66/125 [00:46<00:49,  1.20it/s]\u001b[A\n",
            " 54% 67/125 [00:46<00:42,  1.35it/s]\u001b[A\n",
            " 54% 68/125 [00:47<00:37,  1.54it/s]\u001b[A\n",
            " 55% 69/125 [00:47<00:31,  1.79it/s]\u001b[A\n",
            " 56% 70/125 [00:48<00:28,  1.91it/s]\u001b[A\n",
            " 57% 71/125 [00:48<00:26,  2.01it/s]\u001b[A\n",
            " 58% 72/125 [00:49<00:26,  1.96it/s]\u001b[A\n",
            " 58% 73/125 [00:49<00:24,  2.17it/s]\u001b[A\n",
            " 59% 74/125 [00:49<00:23,  2.18it/s]\u001b[A\n",
            " 60% 75/125 [00:50<00:21,  2.31it/s]\u001b[A\n",
            " 61% 76/125 [00:50<00:22,  2.19it/s]\u001b[A\n",
            " 62% 77/125 [00:51<00:21,  2.20it/s]\u001b[A\n",
            " 62% 78/125 [00:51<00:20,  2.33it/s]\u001b[A\n",
            " 63% 79/125 [00:52<00:20,  2.29it/s]\u001b[A\n",
            " 64% 80/125 [00:52<00:18,  2.39it/s]\u001b[A\n",
            " 65% 81/125 [00:53<00:22,  1.92it/s]\u001b[A\n",
            " 66% 82/125 [00:54<00:27,  1.57it/s]\u001b[A\n",
            " 66% 83/125 [00:55<00:31,  1.35it/s]\u001b[A\n",
            " 67% 84/125 [00:56<00:32,  1.24it/s]\u001b[A\n",
            " 68% 85/125 [00:56<00:30,  1.31it/s]\u001b[A\n",
            " 69% 86/125 [00:57<00:29,  1.33it/s]\u001b[A\n",
            " 70% 87/125 [00:58<00:28,  1.34it/s]\u001b[A\n",
            " 70% 88/125 [00:58<00:26,  1.42it/s]\u001b[A\n",
            " 71% 89/125 [00:59<00:25,  1.41it/s]\u001b[A\n",
            " 72% 90/125 [01:00<00:24,  1.40it/s]\u001b[A\n",
            " 73% 91/125 [01:00<00:23,  1.43it/s]\u001b[A\n",
            " 74% 92/125 [01:01<00:21,  1.55it/s]\u001b[A\n",
            " 74% 93/125 [01:01<00:20,  1.58it/s]\u001b[A\n",
            " 75% 94/125 [01:02<00:19,  1.60it/s]\u001b[A\n",
            " 76% 95/125 [01:03<00:19,  1.50it/s]\u001b[A\n",
            " 77% 96/125 [01:04<00:19,  1.47it/s]\u001b[A\n",
            " 78% 97/125 [01:04<00:19,  1.47it/s]\u001b[A\n",
            " 78% 98/125 [01:05<00:18,  1.44it/s]\u001b[A\n",
            " 79% 99/125 [01:06<00:19,  1.32it/s]\u001b[A\n",
            " 80% 100/125 [01:07<00:20,  1.21it/s]\u001b[A\n",
            " 81% 101/125 [01:08<00:19,  1.22it/s]\u001b[A\n",
            " 82% 102/125 [01:08<00:18,  1.27it/s]\u001b[A\n",
            " 82% 103/125 [01:09<00:16,  1.30it/s]\u001b[A\n",
            " 83% 104/125 [01:10<00:15,  1.31it/s]\u001b[A\n",
            " 84% 105/125 [01:11<00:14,  1.37it/s]\u001b[A\n",
            " 85% 106/125 [01:11<00:13,  1.38it/s]\u001b[A\n",
            " 86% 107/125 [01:12<00:12,  1.39it/s]\u001b[A\n",
            " 86% 108/125 [01:13<00:12,  1.38it/s]\u001b[A\n",
            " 87% 109/125 [01:13<00:11,  1.43it/s]\u001b[A\n",
            " 88% 110/125 [01:14<00:10,  1.42it/s]\u001b[A\n",
            " 89% 111/125 [01:15<00:10,  1.37it/s]\u001b[A\n",
            " 90% 112/125 [01:16<00:09,  1.37it/s]\u001b[A\n",
            " 90% 113/125 [01:16<00:08,  1.44it/s]\u001b[A\n",
            " 91% 114/125 [01:17<00:07,  1.41it/s]\u001b[A\n",
            " 92% 115/125 [01:18<00:07,  1.36it/s]\u001b[A\n",
            " 93% 116/125 [01:19<00:07,  1.27it/s]\u001b[A\n",
            " 94% 117/125 [01:20<00:06,  1.17it/s]\u001b[A\n",
            " 94% 118/125 [01:20<00:05,  1.27it/s]\u001b[A\n",
            " 95% 119/125 [01:21<00:04,  1.29it/s]\u001b[A\n",
            " 96% 120/125 [01:22<00:03,  1.40it/s]\u001b[A\n",
            " 97% 121/125 [01:22<00:02,  1.37it/s]\u001b[A\n",
            " 98% 122/125 [01:23<00:02,  1.33it/s]\u001b[A\n",
            " 98% 123/125 [01:24<00:01,  1.33it/s]\u001b[A\n",
            " 99% 124/125 [01:25<00:00,  1.35it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.5097036361694336, 'eval_bleu': 8.2924, 'eval_gen_len': 9.843, 'eval_runtime': 87.1627, 'eval_samples_per_second': 45.891, 'eval_steps_per_second': 1.434, 'epoch': 3.92}\n",
            " 78% 1200/1530 [13:48<01:07,  4.87it/s]\n",
            "100% 125/125 [01:26<00:00,  1.34it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:24:53,598 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-1200\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:24:53,599 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:24:53,600 >> Configuration saved in ./tmp/tst-translation/checkpoint-1200/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:24:53,601 >> Configuration saved in ./tmp/tst-translation/checkpoint-1200/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:24:58,139 >> Model weights saved in ./tmp/tst-translation/checkpoint-1200/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:24:58,140 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-1200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:24:58,140 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-1200/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:25:02,426 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-1000] due to args.save_total_limit\n",
            "{'loss': 1.7871, 'grad_norm': 3.817655086517334, 'learning_rate': 4.2483660130718954e-06, 'epoch': 4.58}\n",
            " 92% 1400/1530 [14:41<00:26,  4.84it/s][INFO|trainer.py:3363] 2024-03-14 02:25:45,812 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:25:45,812 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:25:45,812 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/125 [00:00<00:44,  2.76it/s]\u001b[A\n",
            "  2% 3/125 [00:01<01:10,  1.74it/s]\u001b[A\n",
            "  3% 4/125 [00:02<01:15,  1.59it/s]\u001b[A\n",
            "  4% 5/125 [00:03<01:19,  1.51it/s]\u001b[A\n",
            "  5% 6/125 [00:03<01:17,  1.53it/s]\u001b[A\n",
            "  6% 7/125 [00:04<01:20,  1.46it/s]\u001b[A\n",
            "  6% 8/125 [00:05<01:22,  1.42it/s]\u001b[A\n",
            "  7% 9/125 [00:05<01:17,  1.49it/s]\u001b[A\n",
            "  8% 10/125 [00:06<01:33,  1.23it/s]\u001b[A\n",
            "  9% 11/125 [00:07<01:25,  1.34it/s]\u001b[A\n",
            " 10% 12/125 [00:08<01:22,  1.36it/s]\u001b[A\n",
            " 10% 13/125 [00:08<01:21,  1.37it/s]\u001b[A\n",
            " 11% 14/125 [00:09<01:20,  1.38it/s]\u001b[A\n",
            " 12% 15/125 [00:10<01:18,  1.40it/s]\u001b[A\n",
            " 13% 16/125 [00:11<01:17,  1.41it/s]\u001b[A\n",
            " 14% 17/125 [00:11<01:24,  1.28it/s]\u001b[A\n",
            " 14% 18/125 [00:12<01:30,  1.18it/s]\u001b[A\n",
            " 15% 19/125 [00:13<01:28,  1.20it/s]\u001b[A\n",
            " 16% 20/125 [00:14<01:24,  1.24it/s]\u001b[A\n",
            " 17% 21/125 [00:15<01:22,  1.26it/s]\u001b[A\n",
            " 18% 22/125 [00:16<01:20,  1.27it/s]\u001b[A\n",
            " 18% 23/125 [00:16<01:17,  1.31it/s]\u001b[A\n",
            " 19% 24/125 [00:17<01:16,  1.33it/s]\u001b[A\n",
            " 20% 25/125 [00:18<01:11,  1.39it/s]\u001b[A\n",
            " 21% 26/125 [00:18<01:11,  1.39it/s]\u001b[A\n",
            " 22% 27/125 [00:19<01:11,  1.37it/s]\u001b[A\n",
            " 22% 28/125 [00:20<01:08,  1.41it/s]\u001b[A\n",
            " 23% 29/125 [00:20<00:56,  1.69it/s]\u001b[A\n",
            " 24% 30/125 [00:21<01:00,  1.57it/s]\u001b[A\n",
            " 25% 31/125 [00:22<01:03,  1.49it/s]\u001b[A\n",
            " 26% 32/125 [00:22<01:03,  1.46it/s]\u001b[A\n",
            " 26% 33/125 [00:23<01:05,  1.41it/s]\u001b[A\n",
            " 27% 34/125 [00:24<01:08,  1.34it/s]\u001b[A\n",
            " 28% 35/125 [00:25<01:04,  1.40it/s]\u001b[A\n",
            " 29% 36/125 [00:25<01:02,  1.43it/s]\u001b[A\n",
            " 30% 37/125 [00:26<01:03,  1.39it/s]\u001b[A\n",
            " 30% 38/125 [00:27<00:57,  1.51it/s]\u001b[A\n",
            " 31% 39/125 [00:27<00:57,  1.50it/s]\u001b[A\n",
            " 32% 40/125 [00:28<00:51,  1.65it/s]\u001b[A\n",
            " 33% 41/125 [00:28<00:45,  1.84it/s]\u001b[A\n",
            " 34% 42/125 [00:28<00:42,  1.97it/s]\u001b[A\n",
            " 34% 43/125 [00:29<00:45,  1.82it/s]\u001b[A\n",
            " 35% 44/125 [00:30<00:50,  1.61it/s]\u001b[A\n",
            " 36% 45/125 [00:31<00:51,  1.54it/s]\u001b[A\n",
            " 37% 46/125 [00:31<00:52,  1.51it/s]\u001b[A\n",
            " 38% 47/125 [00:32<00:54,  1.43it/s]\u001b[A\n",
            " 38% 48/125 [00:33<00:51,  1.50it/s]\u001b[A\n",
            " 39% 49/125 [00:33<00:52,  1.45it/s]\u001b[A\n",
            " 40% 50/125 [00:34<00:54,  1.39it/s]\u001b[A\n",
            " 41% 51/125 [00:35<00:54,  1.36it/s]\u001b[A\n",
            " 42% 52/125 [00:36<00:54,  1.34it/s]\u001b[A\n",
            " 42% 53/125 [00:37<00:55,  1.30it/s]\u001b[A\n",
            " 43% 54/125 [00:38<00:59,  1.20it/s]\u001b[A\n",
            " 44% 55/125 [00:39<01:01,  1.14it/s]\u001b[A\n",
            " 45% 56/125 [00:40<01:02,  1.10it/s]\u001b[A\n",
            " 46% 57/125 [00:40<00:58,  1.17it/s]\u001b[A\n",
            " 46% 58/125 [00:41<00:54,  1.23it/s]\u001b[A\n",
            " 47% 59/125 [00:42<00:51,  1.29it/s]\u001b[A\n",
            " 48% 60/125 [00:42<00:49,  1.31it/s]\u001b[A\n",
            " 49% 61/125 [00:43<00:48,  1.32it/s]\u001b[A\n",
            " 50% 62/125 [00:44<00:46,  1.34it/s]\u001b[A\n",
            " 50% 63/125 [00:45<00:47,  1.31it/s]\u001b[A\n",
            " 51% 64/125 [00:45<00:46,  1.31it/s]\u001b[A\n",
            " 52% 65/125 [00:46<00:45,  1.31it/s]\u001b[A\n",
            " 53% 66/125 [00:47<00:44,  1.33it/s]\u001b[A\n",
            " 54% 67/125 [00:47<00:39,  1.49it/s]\u001b[A\n",
            " 54% 68/125 [00:48<00:34,  1.67it/s]\u001b[A\n",
            " 55% 69/125 [00:48<00:29,  1.91it/s]\u001b[A\n",
            " 56% 70/125 [00:49<00:26,  2.04it/s]\u001b[A\n",
            " 57% 71/125 [00:49<00:25,  2.13it/s]\u001b[A\n",
            " 58% 72/125 [00:50<00:28,  1.84it/s]\u001b[A\n",
            " 58% 73/125 [00:50<00:26,  1.96it/s]\u001b[A\n",
            " 59% 74/125 [00:51<00:28,  1.76it/s]\u001b[A\n",
            " 60% 75/125 [00:51<00:27,  1.84it/s]\u001b[A\n",
            " 61% 76/125 [00:52<00:26,  1.88it/s]\u001b[A\n",
            " 62% 77/125 [00:52<00:24,  1.98it/s]\u001b[A\n",
            " 62% 78/125 [00:53<00:22,  2.13it/s]\u001b[A\n",
            " 63% 79/125 [00:53<00:21,  2.16it/s]\u001b[A\n",
            " 64% 80/125 [00:54<00:19,  2.27it/s]\u001b[A\n",
            " 65% 81/125 [00:54<00:23,  1.89it/s]\u001b[A\n",
            " 66% 82/125 [00:55<00:25,  1.69it/s]\u001b[A\n",
            " 66% 83/125 [00:56<00:27,  1.55it/s]\u001b[A\n",
            " 67% 84/125 [00:57<00:27,  1.47it/s]\u001b[A\n",
            " 68% 85/125 [00:57<00:27,  1.45it/s]\u001b[A\n",
            " 69% 86/125 [00:58<00:27,  1.43it/s]\u001b[A\n",
            " 70% 87/125 [00:59<00:27,  1.40it/s]\u001b[A\n",
            " 70% 88/125 [00:59<00:25,  1.45it/s]\u001b[A\n",
            " 71% 89/125 [01:00<00:24,  1.49it/s]\u001b[A\n",
            " 72% 90/125 [01:01<00:24,  1.44it/s]\u001b[A\n",
            " 73% 91/125 [01:01<00:23,  1.45it/s]\u001b[A\n",
            " 74% 92/125 [01:02<00:23,  1.43it/s]\u001b[A\n",
            " 74% 93/125 [01:03<00:23,  1.34it/s]\u001b[A\n",
            " 75% 94/125 [01:04<00:23,  1.34it/s]\u001b[A\n",
            " 76% 95/125 [01:04<00:22,  1.34it/s]\u001b[A\n",
            " 77% 96/125 [01:05<00:21,  1.33it/s]\u001b[A\n",
            " 78% 97/125 [01:06<00:20,  1.35it/s]\u001b[A\n",
            " 78% 98/125 [01:07<00:19,  1.36it/s]\u001b[A\n",
            " 79% 99/125 [01:07<00:19,  1.36it/s]\u001b[A\n",
            " 80% 100/125 [01:08<00:18,  1.32it/s]\u001b[A\n",
            " 81% 101/125 [01:09<00:17,  1.34it/s]\u001b[A\n",
            " 82% 102/125 [01:10<00:16,  1.36it/s]\u001b[A\n",
            " 82% 103/125 [01:10<00:16,  1.37it/s]\u001b[A\n",
            " 83% 104/125 [01:11<00:15,  1.35it/s]\u001b[A\n",
            " 84% 105/125 [01:12<00:13,  1.46it/s]\u001b[A\n",
            " 85% 106/125 [01:12<00:13,  1.45it/s]\u001b[A\n",
            " 86% 107/125 [01:13<00:12,  1.43it/s]\u001b[A\n",
            " 86% 108/125 [01:14<00:12,  1.32it/s]\u001b[A\n",
            " 87% 109/125 [01:15<00:12,  1.27it/s]\u001b[A\n",
            " 88% 110/125 [01:16<00:12,  1.21it/s]\u001b[A\n",
            " 89% 111/125 [01:17<00:11,  1.24it/s]\u001b[A\n",
            " 90% 112/125 [01:17<00:10,  1.28it/s]\u001b[A\n",
            " 90% 113/125 [01:18<00:08,  1.38it/s]\u001b[A\n",
            " 91% 114/125 [01:19<00:08,  1.36it/s]\u001b[A\n",
            " 92% 115/125 [01:20<00:08,  1.21it/s]\u001b[A\n",
            " 93% 116/125 [01:20<00:07,  1.27it/s]\u001b[A\n",
            " 94% 117/125 [01:21<00:06,  1.31it/s]\u001b[A\n",
            " 94% 118/125 [01:22<00:05,  1.38it/s]\u001b[A\n",
            " 95% 119/125 [01:22<00:04,  1.38it/s]\u001b[A\n",
            " 96% 120/125 [01:23<00:03,  1.45it/s]\u001b[A\n",
            " 97% 121/125 [01:24<00:02,  1.39it/s]\u001b[A\n",
            " 98% 122/125 [01:25<00:02,  1.36it/s]\u001b[A\n",
            " 98% 123/125 [01:25<00:01,  1.37it/s]\u001b[A\n",
            " 99% 124/125 [01:26<00:00,  1.27it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.515702724456787, 'eval_bleu': 8.419, 'eval_gen_len': 9.835, 'eval_runtime': 89.3738, 'eval_samples_per_second': 44.756, 'eval_steps_per_second': 1.399, 'epoch': 4.58}\n",
            " 92% 1400/1530 [16:10<00:26,  4.84it/s]\n",
            "100% 125/125 [01:28<00:00,  1.16it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3054] 2024-03-14 02:27:15,187 >> Saving model checkpoint to ./tmp/tst-translation/checkpoint-1400\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:27:15,188 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:27:15,189 >> Configuration saved in ./tmp/tst-translation/checkpoint-1400/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:27:15,189 >> Configuration saved in ./tmp/tst-translation/checkpoint-1400/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:27:19,929 >> Model weights saved in ./tmp/tst-translation/checkpoint-1400/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:27:19,930 >> tokenizer config file saved in ./tmp/tst-translation/checkpoint-1400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:27:19,930 >> Special tokens file saved in ./tmp/tst-translation/checkpoint-1400/special_tokens_map.json\n",
            "[INFO|trainer.py:3146] 2024-03-14 02:27:24,558 >> Deleting older checkpoint [tmp/tst-translation/checkpoint-1400] due to args.save_total_limit\n",
            "100% 1530/1530 [16:47<00:00,  5.17it/s][INFO|trainer.py:2084] 2024-03-14 02:27:52,009 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "[INFO|trainer.py:2289] 2024-03-14 02:27:52,009 >> Loading best model from ./tmp/tst-translation/checkpoint-1200 (score: 2.5097036361694336).\n",
            "[WARNING|trainer.py:2398] 2024-03-14 02:27:52,259 >> There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "{'train_runtime': 1007.5675, 'train_samples_per_second': 48.488, 'train_steps_per_second': 1.519, 'train_loss': 2.2727644378063725, 'epoch': 5.0}\n",
            "100% 1530/1530 [16:47<00:00,  1.52it/s]\n",
            "[INFO|trainer.py:3054] 2024-03-14 02:27:52,279 >> Saving model checkpoint to ./tmp/tst-translation\n",
            "[WARNING|configuration_utils.py:447] 2024-03-14 02:27:52,279 >> Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
            "[INFO|configuration_utils.py:471] 2024-03-14 02:27:52,281 >> Configuration saved in ./tmp/tst-translation/config.json\n",
            "[INFO|configuration_utils.py:696] 2024-03-14 02:27:52,292 >> Configuration saved in ./tmp/tst-translation/generation_config.json\n",
            "[INFO|modeling_utils.py:2470] 2024-03-14 02:27:54,986 >> Model weights saved in ./tmp/tst-translation/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2470] 2024-03-14 02:27:54,988 >> tokenizer config file saved in ./tmp/tst-translation/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2479] 2024-03-14 02:27:54,988 >> Special tokens file saved in ./tmp/tst-translation/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     2.2728\n",
            "  train_runtime            = 0:16:47.56\n",
            "  train_samples            =       9771\n",
            "  train_samples_per_second =     48.488\n",
            "  train_steps_per_second   =      1.519\n",
            "03/14/2024 02:27:55 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:3363] 2024-03-14 02:27:55,011 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3365] 2024-03-14 02:27:55,012 >>   Num examples = 4000\n",
            "[INFO|trainer.py:3368] 2024-03-14 02:27:55,012 >>   Batch size = 32\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "100% 125/125 [00:39<00:00,  3.18it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_bleu               =     7.5693\n",
            "  eval_gen_len            =    10.0508\n",
            "  eval_loss               =     2.5097\n",
            "  eval_runtime            = 0:00:39.91\n",
            "  eval_samples            =       4000\n",
            "  eval_samples_per_second =    100.217\n",
            "  eval_steps_per_second   =      3.132\n",
            "[INFO|modelcard.py:450] 2024-03-14 02:28:35,272 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Translation', 'type': 'translation'}, 'metrics': [{'name': 'Bleu', 'type': 'bleu', 'value': 7.5693}]}\n"
          ]
        }
      ],
      "source": [
        "!(python3 transformers/examples/pytorch/translation/run_translation.py \\\n",
        "    --model_name_or_path moussaKam/arabart \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --source_lang Darija \\\n",
        "    --target_lang MSA \\\n",
        "    --train_file ./train2.json \\\n",
        "    --validation_file ./test-4000.json \\\n",
        "    --output_dir ./tmp/tst-translation \\\n",
        "    --per_device_train_batch_size=32 \\\n",
        "    --per_device_eval_batch_size=32 \\\n",
        "    --save_steps=200 --eval_steps=200 \\\n",
        "    --logging_steps=200 \\\n",
        "    --save_total_limit=15 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --push_to_hub=False \\\n",
        "    --preprocessing_num_workers=8 \\\n",
        "    --dataloader_num_workers=2 \\\n",
        "    --dataloader_prefetch_factor=2 \\\n",
        "    --num_train_epochs=5 \\\n",
        "    --evaluation_strategy='steps' \\\n",
        "    --save_strategy='steps' \\\n",
        "    --load_best_model_at_end=True \\\n",
        "    --predict_with_generate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}